<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>混合精度训练 › Don&#39;t Respond</title>
  <meta name="author" content="Dou Jiang">
  
  <meta name="description" content="MIXED PRECISION TRAINING
https://arxiv.org/pdf/1710.03740.pdf
论文概述
nvidia的Pascal和Volta系列显卡除了支持标准的单精度计算外，也支持了低精度的计算，比如最新的Tesla
V100硬件支持了FP16的计算加速，P4和P40支持INT8的计算加速，而且低精度计算的峰值要远高于单精浮点的计算峰值。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="混合精度训练"/>
  <meta property="og:site_name" content="Don&#39;t Respond"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Don&#39;t Respond" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  <header id="header"><div class="meta inner">
  <h1><a href="/">Don&#39;t Respond</a></h1>
  <h2><a href="/"></a></h2>
  <nav id="main-nav">
    <ul>
      
      <li><a href="/about">About</a></li>
      
      <li><a href="/archives">Archives</a></li>
      
      <li><a href="/atom.xml">RSS</a></li>
      
    </ul>
    <div class="clearfix"></div>
  </nav>
</div>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  <div class="post-content">
    <header>
      
  
    <h1 class="title">混合精度训练</h1>
  

      
        <time datetime="2018-02-03T04:00:04.000Z">2018-02-03</time>
      
    </header>
    <div class="entry">
      
        <h2 id="mixed-precision-training">MIXED PRECISION TRAINING</h2>
<p><a
target="_blank" rel="noopener" href="https://email.baidu.com/OWA/redir.aspx?C=G_TpaBQZHjfotfty5PDuHfO3av_KUOGPcZOg_60U2vdUx9QS42vVCA..&amp;URL=https%3a%2f%2farxiv.org%2fpdf%2f1710.03740.pdf">https://arxiv.org/pdf/1710.03740.pdf</a></p>
<h3 id="论文概述">论文概述</h3>
<p>nvidia的Pascal和Volta系列显卡除了支持标准的单精度计算外，也支持了低精度的计算，比如最新的Tesla
V100硬件支持了FP16的计算加速，P4和P40支持INT8的计算加速，而且低精度计算的峰值要远高于单精浮点的计算峰值。</p>
<span id="more"></span>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/95247900845ca0aa285aea86b971c6ec.png?raw=true'></p>
<p>为了加速训练过程以及减少显存开销，baidu
Research和nvidia在这篇论文中合作提出了一种FP16和FP32混合精度训练的方法，并且在CNN分类和检测、语音识别和语言模型任务上进行了验证，实验过程中使用的GPU就是Tesla
V100。</p>
<p>训练过程中每层的权重都存成FP32格式（Mater-Weights），每次训练时都会将FP32的权重降精度至FP16（
a master
copy），前向输出和后向梯度都使用FP16进行计算，更新时将FP16的梯度累加到FP32的Mater-Weight上。</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/b89a595f09deb2caf14d44176f931440.png?raw=true'></p>
<h3 id="混合精度的必要性">混合精度的必要性</h3>
<p>由于FP16所能表示的subnormal最小正数是<span
class="math inline">\(2^{−24}\)</span> ≈ <span
class="math inline">\(5.96 × 10^{−8}\)</span>（<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">Half-precision
floating-point format</a>），也就是说在区间（<span
class="math inline">\(-2^{-24},
2^{-24}\)</span>）的数（或者说指数位小于-24的数）使用FP16表示时都会变成0。在一个普通话识别的模型训练中，有将近5%的权重梯度的指数位小于-24，如果更新时也用FP16计算，那么这些数在乘以学习率后都将变成0，从而对最终模型效果产生负面影响，使用混合精度训练的方式可以避免这种问题。</p>
<h3 id="loss-scaling">Loss scaling</h3>
<p>混合精度训练可以解决权重更新量很小的问题，但无法解决梯度本身很小的问题。在一些网络中（比如SSD），梯度大部分都在FP16的表示范围之外，因此需要将梯度平移到FP16的表示范围内
。</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/fc960bb10d950d111404cda831aa5cbe.png?raw=true'></p>
<p>平移实际上就是对梯度值乘以一个系数（等于<span
class="math inline">\(2^{n}\)</span>，<span
class="math inline">\(n\)</span>为平移的位数），但另一种简单高效的方法是直接在前向时就将loss乘以scale，这样在后向传导时所有的梯度都会被乘以相同的scale。权重更新时需要将移位后的梯度除以scale后，再更新到权重上。</p>
<p>论文中提到他们在实验过程中使用的scale是8~32K，最终取得了与FP32一致的收敛结果。对于scale的选择，论文没有统一的方法，只是提到scale并没有下界，只要选择的scale不会在后向计算时导致溢出就行。</p>
<h3 id="实验结果">实验结果</h3>
<ul>
<li><p>图像分类</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/a9166bfb03d36772c83f4aa56e591374.png?raw=true'></p></li>
<li><p>物体检测</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/3dbc1922becd3b150d50bc71aacecb1e.png?raw=true'></p></li>
<li><p>语音识别</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/0369372f891c65571c845b04960aafda.png?raw=true'></p></li>
<li><p>机器翻译</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/237914e80a50fe0f2cac573c36733e5c.png?raw=true'></p></li>
<li><p>语言模型</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/f1c1f41006c8f637c29208ac8652310b.png?raw=true'></p>
<p>​</p></li>
</ul>
<h2
id="mixed-precision-training-of-convolutional-neural-networks-using-integer-operations">MIXED
PRECISION TRAINING OF CONVOLUTIONAL NEURAL NETWORKS USING INTEGER
OPERATIONS</h2>
<p><a
target="_blank" rel="noopener" href="https://email.baidu.com/OWA/redir.aspx?C=a0s4Pl45ENd9uqHgfl_L2eKY-IGy51CKRbN_JHdP0YhUx9QS42vVCA..&amp;URL=https%3a%2f%2fopenreview.net%2fforum%3fid%3dH135uzZ0-">https://openreview.net/forum?id=H135uzZ0-</a></p>
<h3 id="论文概述-1">论文概述</h3>
<p>半精度（16bit）分为半精度浮点（FP16）和半精度定点（INT16），FP16和INT16提供不同的精度和表示范围。INT16相比FP16的动态范围低，但精度更高，因此INT16相比FP16会带来更低的精度误差。</p>
<p>现在深度学习领域公认的数据类型是单精度浮点（float），半精和单精除了在直观感觉上的数据类型不同之外，在计算（algorithmic）和语义（semantic）上也会有很多的不同，比如说FP16的乘加操作得到的结果是FP32。因此在讨论半精度训练时，对于整个tensor的表达、乘加操作、低精度转换、缩放和规整方法和溢出处理都是需要同时考虑的。</p>
<p>intel的这篇论文主要受到之前flexpoint和混合精度训练的启发，从而提出了一种共享指数位的动态定点表达（dynamic
fixed point
representation）方法，使用INT16和float混合精度训练，在完全不进行任何调参的情况下，在多个CNN的模型上取得了当前所有低精度训练方法中最好的效果。</p>
<p>这篇论文主要涉及的技术点有：</p>
<ul>
<li>DFP：INT16的Tensor共享指数位，扩充INT16的动态表示范围。</li>
<li>instruction：两个INT16进行乘法，结果存为INT32的指令。</li>
<li>down-convert：基于最大值的低精度转换策略，使用nearest、stochastic和biased
rounding三种不同的rounding方法。</li>
<li>overflow
management：将局部的INT32结果累加到FP32，防止累加时溢出。</li>
</ul>
<h3 id="dfpdynamic-fixed-point">DFP（Dynamic Fixed Point）</h3>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/f54c9019a7174299761d48094d1f0dab.png?raw=true'></p>
<p>一个DFP
tensor由一个定点的tensor和该tensor共享的指数组成，更通用的表示形式为DFP-P
= <span class="math inline">\(&lt;I, E_{s}&gt;\)</span>，P表示定点tensor
<span class="math inline">\(I\)</span>的位宽，<span
class="math inline">\(E_{s}\)</span>表示共享指数位。标准单精使用的是8bit的指数位，在该论文中使用的DFP-16共享指数位也是8bit。</p>
<ul>
<li><p>DFP-16和fp32的数据转换</p>
<p>共享指数位需要根据tensor中的绝对值最大的数和定点化的位宽来确定，计算公式如下：</p>
<p><span class="math display">\[E_{s} = E_{fmax} - (P - 2)\]</span></p>
<p><span class="math inline">\(E_{s}\)</span>表示DFP-P的共享指数，<span
class="math inline">\(E_{fmax}\)</span>表示原始fp32
tensor中绝对值最大的数对应的指数<span class="math inline">\(E_{fmax} =
E(max_{\forall f \in F} |f|)\)</span></p>
<p>因此fp32的tensor与DFP的tensor有以下关系：</p>
<p><span class="math display">\[\forall i_{n} \in I, \ \ \ f_{n} = i_{n}
\times 2^{E_{s}}, \ \ \ where f_{n} \in F\]</span></p>
<p>也就是说<span class="math inline">\(i_{n} =
rounding(\frac{f_{n}}{2^{E_{s}}})\)</span>，这本质上与loss
scaling思想是一样的，用平移的思想来解决动态范围不够的问题。</p></li>
<li><p>DFP-16 tensor的乘加运算规则</p>
<p>1、两个DFP-16 tensor相乘，结果存为DFP-32。</p>
<p><span class="math display">\[i_{ab} = i_{a} \times i_{b} , \ \ \
E_{s}^{ab} = E_{s}^{a} + E_{s}^{b}\]</span></p>
<p>2、两个DFP-16 tensor相加，结果存为DFP-32。</p>
<p><span class="math display">\[i_{ab} = \left\{\begin{aligned} i_{a} +
(i_{b} &gt;&gt; (E_{s}^{a} - E_{s}^{b})) \ \ \ when E_{s}^{a} &gt;
E_{s}^{b} \\ i_{b}+(i_{a} &gt;&gt; (E_{s}^{b}-E_{s}^{a})) \ \ \ when
E_{s}^{a} &lt; E_{s}^{b} \end{aligned}\right.\]</span></p>
<p><span class="math display">\[E_{s}^{a+b} = max(E_{s}^{a},
E_{s}^{b})\]</span></p>
<p>3、两个DFP-32 tensor相加，结果保存为fp32。</p></li>
<li><p>DFP-32和DFP-16的数据转换</p>
<p><span class="math display">\[R_{s} = P - LZC(max_{\forall i_{ab} \in
I^{32}}|i_{ab}|)\]</span></p>
<p><span class="math display">\[i_{ab}^{d} = i_{ab} &gt;&gt; R_{s} , \ \
\ E_{s}^{ab} += R_{s}\]</span></p></li>
</ul>
<h3 id="dfp混合精度训练">DFP混合精度训练</h3>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/9b304e74b8dbc9ec6324c56d05b85f24.png?raw=true'></p>
<h3 id="指令实现">指令实现</h3>
<p>intel的VNNI指令集中有一条DFP-16乘加的指令QVNNI16，这条指令的第一个操作数是DFP-16内存指针，第二个操作数是4个512位的向量寄存器（每个寄存器可以存储32个DFP-16），结果是一个512位的向量寄存器（该寄存器能存储16个DFP-32）。</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/4a18c89da9676673a73c899987564e10.png?raw=true'></p>
<p>上面的QVNNI16指令集实际上对mem输入做了两路并行展开，vinp2中一个寄存器支持同时对输入feature
map的两个channel进行计算。在论文中，卷积层输入的格式为（N，C/16，H，W，16），权重的格式为（C/16，K/16，KH，KW，8c，16k，2c），C表示输入feature
map的通道数，K表示输出通道数，KH和KW分别表示卷积核的height和width。</p>
<p>卷积计算过程伪代码：</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/2f5405a955c03cd522b5b1f17e7300cd.png?raw=true'></p>
<p>每次对输入的ICBLK个通道进行计算，ICBLK个通道又会分成（ICBLK/16）组，每组计算16个通道，由于QVNNI指令每次只能对输入的8个通道进行计算，因此每组调用2次QVNNI16指令，计算结果vout会转换成FP32后与output累加。</p>
<h3 id="实验结果-1">实验结果</h3>
<p>baseline和DFP-16的实验均在intel最新的Knights-Mill
CPU上进行，DFP-16相比FP32训练加速1.8X。</p>
<p><img src='https://github.com/hjchen2/personal/blob/master/blog/mixed-precision/55d321517c2de03fe92f7c32aff1d87a.png?raw=true'></p>
<h3 id="abs_max量化方案">ABS_MAX量化方案</h3>
<h3 id="dfp与abs_max量化的区别">DFP与ABS_MAX量化的区别</h3>

      
    </div>
      
      <footer>
        
  
  <div class="categories">
    <a href="/categories/low-bitwidth/">low bitwidth</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/int16/">int16</a>, <a href="/tags/fp16/">fp16</a>, <a href="/tags/混合精度训练/">混合精度训练</a>, <a href="/tags/loss-scaling/">loss scaling</a>, <a href="/tags/QVNNI16/">QVNNI16</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
      </footer>
  </div>
</article>


<section id="comment">
  
</section>


</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:hjchen2.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Recent Posts</h3>
  <ul class="entry">
    
      <li>
        <a href="/2023/02/24/IREE编译流程6/">IREE编译流程解析(六)</a>
      </li>
    
      <li>
        <a href="/2023/02/13/IREE编译流程5/">IREE编译流程解析(五)</a>
      </li>
    
      <li>
        <a href="/2023/01/04/IREE编译流程4/">IREE编译流程解析(四)</a>
      </li>
    
      <li>
        <a href="/2023/01/04/IREE编译流程3/">IREE编译流程解析(三)</a>
      </li>
    
      <li>
        <a href="/2023/01/04/IREE编译流程2/">IREE编译流程解析(二)</a>
      </li>
    
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/DL-Compiler/">DL Compiler</a><small>8</small></li>
  
    <li><a href="/categories/Daily/">Daily</a><small>1</small></li>
  
    <li><a href="/categories/ML-framework/">ML framework</a><small>2</small></li>
  
    <li><a href="/categories/XRT/">XRT</a><small>1</small></li>
  
    <li><a href="/categories/code/">code</a><small>1</small></li>
  
    <li><a href="/categories/deep-learning/">deep learning</a><small>1</small></li>
  
    <li><a href="/categories/graph-optimization-图优化/">graph optimization, 图优化</a><small>1</small></li>
  
    <li><a href="/categories/kaldi-decision-tree-决策树/">kaldi, decision tree, 决策树</a><small>1</small></li>
  
    <li><a href="/categories/low-bitwidth/">low bitwidth</a><small>1</small></li>
  
    <li><a href="/categories/model-compression/">model compression</a><small>1</small></li>
  
    <li><a href="/categories/neural-machine-translation/">neural machine translation</a><small>1</small></li>
  
    <li><a href="/categories/reinforcement-learning/">reinforcement learning</a><small>3</small></li>
  
    <li><a href="/categories/tvm-knowledge/">tvm knowledge</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Compiler/" style="font-size: 10px;">Compiler</a> <a href="/tags/Deep-Learning-Compiler/" style="font-size: 20px;">Deep Learning Compiler</a> <a href="/tags/Encoder-Decoder/" style="font-size: 10px;">Encoder-Decoder</a> <a href="/tags/FusionStitching/" style="font-size: 10px;">FusionStitching</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IREE/" style="font-size: 17.5px;">IREE</a> <a href="/tags/KunPeng/" style="font-size: 10px;">KunPeng</a> <a href="/tags/PackedFunc/" style="font-size: 10px;">PackedFunc</a> <a href="/tags/QVNNI16/" style="font-size: 10px;">QVNNI16</a> <a href="/tags/TVM/" style="font-size: 10px;">TVM</a> <a href="/tags/TensorFlow-XLA/" style="font-size: 10px;">TensorFlow XLA</a> <a href="/tags/TensorRT/" style="font-size: 10px;">TensorRT</a> <a href="/tags/XLA/" style="font-size: 10px;">XLA</a> <a href="/tags/XRT/" style="font-size: 10px;">XRT</a> <a href="/tags/c/" style="font-size: 10px;">c++</a> <a href="/tags/caffe/" style="font-size: 12.5px;">caffe</a> <a href="/tags/decision-tree/" style="font-size: 10px;">decision tree</a> <a href="/tags/deep-learning/" style="font-size: 12.5px;">deep learning</a> <a href="/tags/embedding/" style="font-size: 10px;">embedding</a> <a href="/tags/fp16/" style="font-size: 10px;">fp16</a> <a href="/tags/framework/" style="font-size: 10px;">framework</a> <a href="/tags/graph-optimization/" style="font-size: 10px;">graph optimization</a> <a href="/tags/int16/" style="font-size: 10px;">int16</a> <a href="/tags/kaldi/" style="font-size: 10px;">kaldi</a> <a href="/tags/large-scale-ML-framework/" style="font-size: 10px;">large scale ML framework</a> <a href="/tags/loss-scaling/" style="font-size: 10px;">loss scaling</a> <a href="/tags/machine-learning/" style="font-size: 12.5px;">machine learning</a> <a href="/tags/machine-learning%EF%BC%8C%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" style="font-size: 10px;">machine learning，贝尔曼公式推导</a> <a href="/tags/machine-translation/" style="font-size: 10px;">machine translation</a> <a href="/tags/momentum/" style="font-size: 10px;">momentum</a> <a href="/tags/pruning/" style="font-size: 10px;">pruning</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/reinforcement-learning/" style="font-size: 15px;">reinforcement learning</a> <a href="/tags/seq2seq/" style="font-size: 10px;">seq2seq</a> <a href="/tags/substitution/" style="font-size: 10px;">substitution</a> <a href="/tags/super-optimization/" style="font-size: 10px;">super optimization</a> <a href="/tags/web-technology/" style="font-size: 10px;">web technology</a> <a href="/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9B%B8%E5%85%B3%E9%9F%B3%E7%B4%A0/" style="font-size: 10px;">上下文相关音素</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" style="font-size: 10px;">决策树</a> <a href="/tags/%E5%9B%BE%E6%9B%BF%E6%8D%A2/" style="font-size: 10px;">图替换</a> <a href="/tags/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">混合精度训练</a> <a href="/tags/%E8%B6%85%E4%BC%98%E5%8C%96/" style="font-size: 10px;">超优化</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2023 Dou Jiang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



</body>
</html>

