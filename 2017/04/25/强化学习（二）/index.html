
 <!DOCTYPE HTML>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
  
    <title>强化学习（二） | Don&#39;t Respond</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Dou Jiang">
    

    
    <meta name="description" content="DQN 前面我们讲到TD算法结合了动态规划和蒙特卡洛算法的优点，不依赖具体的环境模型，并且更新时采用滑动平均的方式，因此单步就能更新，而不需要生成整个episode，在非episode情况下仍然适用。TD算法又分为on policy的sarsa算法和off policy的Q learning算法，其中Q learning算法直接使用下一状态的最大动作值函数进行更新，加快了算法收敛速度，因此Q le">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习（二）">
<meta property="og:url" content="https://hjchen2.github.io/2017/04/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/index.html">
<meta property="og:site_name" content="Don&#39;t Respond">
<meta property="og:description" content="DQN 前面我们讲到TD算法结合了动态规划和蒙特卡洛算法的优点，不依赖具体的环境模型，并且更新时采用滑动平均的方式，因此单步就能更新，而不需要生成整个episode，在非episode情况下仍然适用。TD算法又分为on policy的sarsa算法和off policy的Q learning算法，其中Q learning算法直接使用下一状态的最大动作值函数进行更新，加快了算法收敛速度，因此Q le">
<meta property="og:locale">
<meta property="og:image" content="https://raw.githubusercontent.com/hjchen2/personal/master/blog/pictures/9930b76dc4a4c37e188ea6363fe6603b.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/pictures/30EFF3D4-0562-4544-BFF9-D43B3EC7AFF7.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/pictures/8e238f9d9836b789276e0e58d4aa1e34.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/pictures/93F5C516-8E53-4F89-B03E-3EDD95DF1C76.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/pictures/1E5C7D95-519A-4B54-BF09-C27A163D12C8.png?raw=true">
<meta property="article:published_time" content="2017-04-25T04:31:08.000Z">
<meta property="article:modified_time" content="2023-05-19T03:51:57.657Z">
<meta property="article:author" content="Dou Jiang">
<meta property="article:tag" content="reinforcement learning">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/hjchen2/personal/master/blog/pictures/9930b76dc4a4c37e188ea6363fe6603b.png?raw=true">

    
    <link rel="alternative" href="/atom.xml" title="Don&#39;t Respond" type="application/atom+xml">
    
    
    
    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Don&#39;t Respond">Don&#39;t Respond</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:hjchen2.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/25/强化学习（二）/" title="强化学习（二）" itemprop="url">强化学习（二）</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Dou Jiang" target="_blank" itemprop="author">Dou Jiang</a>
		
  <p class="article-time">
    <time datetime="2017-04-25T04:31:08.000Z" itemprop="datePublished"> Published 2017-04-25</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#dqn"><span class="toc-number">1.</span> <span class="toc-text">DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#q-learning%E4%BE%8B%E5%AD%90"><span class="toc-number">1.1.</span> <span class="toc-text">Q learning例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91"><span class="toc-number">1.2.</span> <span class="toc-text">函数逼近</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dqn-1"><span class="toc-number">1.3.</span> <span class="toc-text">DQN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="toc-number">1.4.</span> <span class="toc-text">经验回放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.5.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gym%E4%BD%BF%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">Gym使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gym%E7%AE%80%E4%BB%8B"><span class="toc-number">2.1.</span> <span class="toc-text">Gym简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAgym%E7%8E%AF%E5%A2%83"><span class="toc-number">2.2.</span> <span class="toc-text">创建一个Gym环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#step"><span class="toc-number">2.3.</span> <span class="toc-text">step</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reset"><span class="toc-number">2.4.</span> <span class="toc-text">reset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#render"><span class="toc-number">2.5.</span> <span class="toc-text">render</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spaces"><span class="toc-number">2.6.</span> <span class="toc-text">Spaces</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#breakout-v0%E4%BE%8B%E5%AD%90"><span class="toc-number">2.7.</span> <span class="toc-text">Breakout-v0例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">3.</span> <span class="toc-text">参考资料</span></a></li></ol>
		
		</div>
		
		<h2 id="dqn">DQN</h2>
<p>前面我们讲到TD算法结合了动态规划和蒙特卡洛算法的优点，不依赖具体的环境模型，并且更新时采用滑动平均的方式，因此单步就能更新，而不需要生成整个episode，在非episode情况下仍然适用。TD算法又分为on
policy的sarsa算法和off policy的Q learning算法，其中Q
learning算法直接使用下一状态的最大动作值函数进行更新，加快了算法收敛速度，因此Q
learning算法在实际应用中更加普遍。</p>
<span id="more"></span>
<h3 id="q-learning例子">Q learning例子</h3>
<p>我们用一个例子来说明Q
learning算法的过程。下图是一个二叉树表示的路径规划问题，每一个节点代表环境中的一个状态，叶子节点表示终止状态，每个非叶子节点都可以选择向上或向下的动作，然后转移到下一个节点，并获得相应的得分。</p>
<div data-align="center">
<img src="https://raw.githubusercontent.com/hjchen2/personal/master/blog/pictures/9930b76dc4a4c37e188ea6363fe6603b.png?raw=true" width=600>
</div>
<p></br></p>
<p>首先初始化所有状态动作对的动作值函数：<span
class="math inline">\(Q(S_{i},a)=0, \forall i\in[1，6],a\in[上,
下]\)</span>，并且初始化<span class="math inline">\(\epsilon =
0.1，\alpha = 0.1\)</span>。</p>
<ul>
<li><p>随机选择一个初始状态<span
class="math inline">\(S\)</span>，假设为<span
class="math inline">\(S_0\)</span><br />
根据<span
class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到状态<span
class="math inline">\(S_1\)</span>，那么更新<span
class="math inline">\(Q(S_0,上)=Q(S_0,上)+\alpha\cdot(R_{1}+\max_aQ(S_1,a)-Q(S_0,上))=0+0.1\cdot(10+0-0)=1\)</span>，接下来继续根据<span
class="math inline">\(\epsilon-greedy\)</span>策略选择下一个动作，比如下，并且转移到终止状态<span
class="math inline">\(S_4\)</span>，因此<span
class="math inline">\(Q(S_1,下)=Q(S_0,下)+\alpha\cdot(R_{2}+\max_aQ(S_4,a)-Q(S_1,下))=0+0.1\cdot(100+0-0)=10\)</span>。</p></li>
<li><p>随机选择一个初始状态<span
class="math inline">\(S\)</span>，假设为<span
class="math inline">\(S_2\)</span><br />
根据<span
class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到终止状态<span
class="math inline">\(S_5\)</span>，则更新<span
class="math inline">\(Q(S_2,上)=0+0.1\cdot（100+0-0）=10\)</span></p></li>
<li><p>随机选择一个初始状态<span
class="math inline">\(S\)</span>，假设为<span
class="math inline">\(S_0\)</span><br />
根据<span
class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到状态<span
class="math inline">\(S_1\)</span>，则更新<span
class="math inline">\(Q(S_0,上)=1+0.1\cdot(10+10-1)=2.9\)</span>，选择下一个动作，比如上，则<span
class="math inline">\(Q(S_1,上)=0+0.1\cdot(50+0-0)=5\)</span></p></li>
<li><p>随机选择一个初始状态<span
class="math inline">\(S\)</span>，假设为<span
class="math inline">\(S_0\)</span><br />
根据<span
class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到状态<span
class="math inline">\(S_1\)</span>，则更新<span
class="math inline">\(Q(S_0,上)=2.9+0.1\cdot(10+10-2.9)=4.61\)</span>，选择下一个动作，比如下，则<span
class="math inline">\(Q(S_1,下)=10+0.1\cdot(100+0-10)=19\)</span></p></li>
<li><p>…</p></li>
</ul>
<p>下面是该例子的python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">author: Houjiang Chen</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">q_learning</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, states, actions</span>):</span><br><span class="line">        self.states = states</span><br><span class="line">        self.actions = actions</span><br><span class="line">        self.eps = <span class="number">0.1</span></span><br><span class="line">        self.alpha = <span class="number">0.1</span></span><br><span class="line">        self.q_table = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(actions)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(states)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_action</span>(<span class="params">self, current_state</span>):</span><br><span class="line">        max_action = self.q_table[current_state].index(<span class="built_in">max</span>(self.q_table[current_state]))</span><br><span class="line">        <span class="keyword">if</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &gt; self.eps:</span><br><span class="line">            <span class="keyword">return</span> max_action</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rest = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.q_table[current_state])) <span class="keyword">if</span> i != max_action]</span><br><span class="line">            index = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(rest) - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> rest[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, current_state, action, next_state, reward, final</span>):</span><br><span class="line">        <span class="keyword">if</span> final != <span class="number">1</span>:</span><br><span class="line">            reward = reward + <span class="built_in">max</span>(self.q_table[next_state])</span><br><span class="line">        self.q_table[current_state][action] += self.alpha * (reward - self.q_table[current_state][action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">environment</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.level = <span class="number">2</span></span><br><span class="line">        self.actions = <span class="number">2</span></span><br><span class="line">        self.states = self.actions ** (self.level + <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">        self.final_states = self.actions ** self.level</span><br><span class="line">        self.reward = &#123;<span class="number">0</span> : [<span class="number">10</span>, -<span class="number">10</span>], <span class="number">1</span> : [<span class="number">50</span>, <span class="number">100</span>], <span class="number">2</span> : [<span class="number">100</span>, <span class="number">150</span>]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self, current_state, action</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;action: 0 or 1</span></span><br><span class="line"><span class="string">           return: next_state, reward, is_final</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">next</span> = <span class="number">2</span> * current_state + (action + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">next</span> &gt;= self.states - self.final_states:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span>, self.reward[current_state][action], <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">next</span>, self.reward[current_state][action], <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, self.states - self.final_states - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">env = environment()</span><br><span class="line">agent = q_learning(env.states, env.actions)</span><br><span class="line"></span><br><span class="line">episode = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> episode &lt; <span class="number">100000</span>:</span><br><span class="line">    episode += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;episode: %d&quot;</span> % episode</span><br><span class="line">    current_state = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        action = agent.get_action(current_state)</span><br><span class="line">        next_state, reward, final = env.<span class="built_in">next</span>(current_state, action)</span><br><span class="line">        agent.update(current_state, action, next_state, reward, final)</span><br><span class="line">        <span class="keyword">if</span> final:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        current_state = next_state</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> agent.q_table</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p>最终收敛结果为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">109.99999999999989</span>, <span class="number">139.99999999999977</span>], </span><br><span class="line">[<span class="number">49.99999999999997</span>, <span class="number">99.99999999999994</span>], </span><br><span class="line">[<span class="number">99.99999999999994</span>, <span class="number">149.9999999999999</span>], </span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="函数逼近">函数逼近</h3>
<p>上面的例子中非终止状态数只有3个，每个非终止状态对应的动作只有2个，因此状态动作对总共有6个，使用表格存储完全没有问题，但实际上我们需要解决的并不是一个如此简单的问题。比如在【Playing
Atari with Deep Reinforcement Learning】中DeepMind就使用Q
learning使得agent玩Atari 2600游戏的水平超越了人类水平。在Atari
2600游戏中，每个游戏画面都是一个状态，如果每个画面都是像素为84*84的256灰度图像，那么将会产生<span
class="math inline">\(256^{84\cdot84}\)</span>个状态，用表格进行存储将会变得非常不现实。为了解决状态数爆炸的问题，通常可以使用函数逼近的方法。下面有几种函数表示的方式：</p>
<div data-align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/30EFF3D4-0562-4544-BFF9-D43B3EC7AFF7.png?raw=true" width=600>
</div>
<p></br></p>
<p>并且逼近函数的形式可以采用：</p>
<ul>
<li>Linear combinations of features</li>
<li>Neural network</li>
<li>Decision tree</li>
<li>Nearest neighbour</li>
<li>Fourier / wavelet bases</li>
<li>...</li>
</ul>
<p>下面我们研究的DQN（Deep Q Network）就是采用Deep neural
network进行动作值函数逼近的一种方法，结构如下。</p>
<div data-align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/8e238f9d9836b789276e0e58d4aa1e34.png?raw=true" width=400>
</div>
<p></br></p>
<p>为推导方便，假设中间的Network为一层的全连接，即<span
class="math inline">\(\hat{V}(s,
a)=x(S)^{T}w=\sum_{j=1}^{n}{x_{j}(S)w_{j}}​\)</span>，代价函数选择最小均方误差：<span
class="math inline">\(J(w)=\frac{1}{2}(V(s,a)-\hat{V}(s,a))^2​\)</span>，采用随机梯度下降算法进行优化。</p>
<p><span
class="math display">\[\begin{split}\frac{\partial{J(w)}}{\partial{w}}&amp;=\left(V(s,a)-\hat{V}(s,a)\right)\frac{\partial{\hat{V}(s,a)}}
{\partial{w}} \\ &amp;=\left(V(s,a)-\hat{V}(s,a)\right)x(S)
\end{split}\tag{1-1}\]</span></p>
<p><span class="math display">\[\begin{split}w^k&amp;=w^{k-1}+\eta
\Delta(w)\\&amp;=w^{k-1}-\eta
\frac{\partial{J(w)}}{\partial{w}}\\&amp;=w^{k-1}-\eta
\left(V(s,a)-\hat{V}(s,a;w^{k})\right)x(S)\end{split}\tag{1-2}\]</span></p>
<p>由于我们并没有动作值函数的真实值，因此与Q learning类似，<span
class="math inline">\(V(s,a,)\)</span>可以使用下一个状态的动作值函数进行估计，即<span
class="math inline">\(V(s,a)=V(s,a;w^{k-1})=r+\gamma
\max_{a^{&#39;}}V(s^{&#39;},a^{&#39;};w^{k-1})\)</span>。</p>
<p>整个训练过程仍然与Q learning一样，采用<span
class="math inline">\(\epsilon-greedy\)</span>策略选择动作，并按照公式(1-2)更新权重<span
class="math inline">\(w\)</span>，实际上也就更新了策略的动作值函数。使用值函数逼近的方法不需要枚举每个状态动作对，突破了状态数的限制，使得Q
learning在一些复杂任务上得到广泛应用，但仍然没有解决动作数爆炸或者连续动作的问题。</p>
<h3 id="dqn-1">DQN</h3>
<p>DQN最先出现于DeepMind发表的【Playing Atari with Deep Reinforcement
Learning】论文中，由于需要直接输入图像画面，因此论文中使用CNN来表示Q函数，下面简单剖析一下该论文。</p>
<p>使用的是典型的CNN，其结构为：</p>
<div data-align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/93F5C516-8E53-4F89-B03E-3EDD95DF1C76.png?raw=true" width=600>
</div>
<p></br>
与一般的CNN有所不同的是，没有pooling层，因为我们这里不是做图像分类，pooling层带来的旋转和数值不变性对分类是有作用的，但在这个任务中对物体的具体位置是非常敏感的，因此移除了pooling层。</p>
<p>Atari原始的游戏帧为210*160像素的RGB图像，由于该任务对画面色彩不敏感，为了减少计算开销，将游戏帧预处理成84*84的灰度图像。但为了获得动态特征，最终是将前3帧图像与当前帧stack到一起组成一个4*84*84的图像作为CNN的输入，输出为每个动作对应的Q值。</p>
<h3 id="经验回放">经验回放</h3>
<p>现在我们知道可以使用Q
learning去估计每个状态的未来回报的期望，并且可以使用CNN去逼近动作值函数，也就是可以使用DQN去解决一个复杂的MDP任务。但在实际应用时会出现更新波动较大，导致收敛非常慢的问题，DeepMind因此使用了一个经验回放（Experience
Replay）机制，就是将每步的经验数据<span
class="math inline">\(&lt;s,a,r,s^{&#39;}&gt;\)</span>存放在回放内存中，更新时都从回放内存中随机采样一个batch的数据进行更新。</p>
<p>经验回放机制相比标准的DQN有两个好处：首先每一步的经验数据会被保存起来，更新时可以多次使用到经验数据，使得数据利用更高效；此外直接从连续的样本中学习是低效的，因为一个episode内样本具有很强的相关性，随机挑选样本打破了这种相关性，因此减小了更新时的变化，使得更新更加稳定（注：因为同一次实验过程的样本相关性很强，不同实验之间的相关性就显得相对比较小，如果使用连续的样本进行训练，在切换到下一次实验的样本时会导致模型更新不稳定）。</p>
<p>由于内存大小限制，回放内存不可能将所有的经验数据都保存起来，因此只会保留最新的N组经验数据，比较久远的数据就会被遗忘。</p>
<h3 id="训练">训练</h3>
<p>DeepMind使用DQN对
ATARI中七个游戏进行了实验，由于每个游戏的得分尺度不一致，因此他们将得分分为正回报、负回报和无回报，正回报得分为1，负回报得分为-1，无回报得分为0。</p>
<p>使用 RMSProp算法进行优化，batch size为32，采用<span
class="math inline">\(\epsilon-greedy\)</span>行动策略，前一百万帧的<span
class="math inline">\(\epsilon\)</span>从1线性减少到0.1，最后固定为0.1。总共训练了一千万帧，并且使用了一百万大小的回放内存。</p>
<p>训练过程伪代码：</p>
<div data-align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/1E5C7D95-519A-4B54-BF09-C27A163D12C8.png?raw=true" width=600>
</div>
<h2 id="gym使用">Gym使用</h2>
<h3 id="gym简介">Gym简介</h3>
<p>目前强化学习的研究主要由DeepMind和OpenAI两家在主导，去年底到今年初DeepMind和OpenAI相继开源了自家的3D
learning environment平台DeepMind Lab和Universe。DeepMind
Lab目前给出的文档和例子都比较少，使用也稍显复杂，所以暂时可以不考虑使用。Universe包含了1000+的游戏环境，并且将程序打包在docker环境中运行，提供与Gym一致的接口。Universe的环境由一个client和一个remote组成，client是一个VNCenv，主要负责接收agent的动作，传递回报和管理本地episode的状态，remote是指在docker环境中运行的程序，remote可以运行在本地、远程服务器或在cloud上。client和remote通过VNC远程桌面系统进行交互，通过WebSocket传递回报、诊断和控制信息。</p>
<p>由于Universe环境提供Gym接口，而Gym是OpenAI去年4月份发布的一套开发和比较强化学习算法的toolkit。Gym本身是可以独立于Universe使用的，并且Universe和Gym中agent代码基本没有什么区别。我们下面就单独讲讲Gym接口和如何使用Gym训练自己的agent。</p>
<p>Gym目前提供python接口，并支持任何的计算框架，比如tensorflow、theano等。强化学习解决的是agent和环境交互的任务，agent根据当前环境状态做出某个动作，然后观察下一个状态和回报，环境根据agent的动作转移到下一个状态，并发送回报。Gym提供的实际上是环境这个角色，每个Gym环境都提供一致的接口。</p>
<h3 id="创建一个gym环境">创建一个Gym环境</h3>
<p>创建一个环境时只需要指定环境id，比如agent需要玩Atari
Breakout-v0这个游戏，可以如下创建一个Breakout-v0的环境。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;Breakout-v0&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="step">step</h3>
<p>输入agent的动作，返回4个值，分别为：</p>
<ul>
<li>observation：表示agent观察到的下一个状态，比如在一些游戏中，observation为RGB的图像</li>
<li>reward：表示执行输入的动作后得到的回报值</li>
<li>done：表示返回的observation是不是结束状态</li>
<li>info：调试信息，一般没什么用处</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next_state, reward, terminal, _ = env.step(action)</span><br></pre></td></tr></table></figure>
<h3 id="reset">reset</h3>
<p>在开始一个新的episode时，Gym环境都要reset，获得一个初始状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init_state = env.reset()</span><br></pre></td></tr></table></figure>
<h3 id="render">render</h3>
<p>render是Gym用来渲染环境状态的函数，当调用该函数时会出现一个动图框。一般agent执行一个动作，环境都要渲染一次，这样就可以实时看到agent的执行情况了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.render()</span><br></pre></td></tr></table></figure>
<h3 id="spaces">Spaces</h3>
<p>Gym环境有两个space属性，一个是action_space，一个是observation_space，分别表示该Gym环境下合法的动作和状态。action_space是Gym中的一个Discrete对象，Discrete对象有一个成员n，表示合法的动作数，比如Discrete(2)表示有两个合法动作，编号从0开始，因此两个动作编号为0和1。observation_space是Gym中的一个Box对象，Box的shape表示observation的数据组织方式，比如Box(210,
160,
3)表示合法的observation是一个210*160*3的数组，而Box(4,)表示observation是一个大小为4的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">observation_space = env.observation_space <span class="comment"># observation_space: Discrete(6)</span></span><br><span class="line">action_space = env.action_space <span class="comment"># action_space: Box(210, 160, 3)</span></span><br></pre></td></tr></table></figure>
<h3 id="breakout-v0例子">Breakout-v0例子</h3>
<p>采用了github上Flood Sung的DQN实现，感谢Flood Sung大神的无私贡献。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------------------------</span></span><br><span class="line"><span class="comment"># File: Deep Q-Learning Algorithm</span></span><br><span class="line"><span class="comment"># Author: Flood Sung</span></span><br><span class="line"><span class="comment"># Date: 2016.3.21</span></span><br><span class="line"><span class="comment"># -----------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters:</span></span><br><span class="line">FRAME_PER_ACTION = <span class="number">1</span></span><br><span class="line">GAMMA = <span class="number">0.99</span> <span class="comment"># decay rate of past observations</span></span><br><span class="line">OBSERVE = <span class="number">100.</span> <span class="comment"># timesteps to observe before training</span></span><br><span class="line">EXPLORE = <span class="number">200000.</span> <span class="comment"># frames over which to anneal epsilon</span></span><br><span class="line">FINAL_EPSILON = <span class="number">0</span><span class="comment">#0.001 # final value of epsilon</span></span><br><span class="line">INITIAL_EPSILON = <span class="number">0</span><span class="comment">#0.01 # starting value of epsilon</span></span><br><span class="line">REPLAY_MEMORY = <span class="number">50000</span> <span class="comment"># number of previous transitions to remember</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment"># size of minibatch</span></span><br><span class="line">UPDATE_TIME = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BrainDQN</span>:</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,actions</span>):</span><br><span class="line">		<span class="comment"># init replay memory</span></span><br><span class="line">		self.replayMemory = deque()</span><br><span class="line">		<span class="comment"># init some parameters</span></span><br><span class="line">		self.timeStep = <span class="number">0</span></span><br><span class="line">		self.epsilon = INITIAL_EPSILON</span><br><span class="line">		self.actions = actions</span><br><span class="line">		<span class="comment"># init Q network</span></span><br><span class="line">		self.stateInput,self.QValue,self.W_conv1,self.b_conv1,self.W_conv2,self.b_conv2,self.W_conv3,self.b_conv3,self.W_fc1,self.b_fc1,self.W_fc2,self.b_fc2 = self.createQNetwork()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># init Target Q Network</span></span><br><span class="line">		self.stateInputT,self.QValueT,self.W_conv1T,self.b_conv1T,self.W_conv2T,self.b_conv2T,self.W_conv3T,self.b_conv3T,self.W_fc1T,self.b_fc1T,self.W_fc2T,self.b_fc2T = self.createQNetwork()</span><br><span class="line"></span><br><span class="line">		self.copyTargetQNetworkOperation = [self.W_conv1T.assign(self.W_conv1),self.b_conv1T.assign(self.b_conv1),self.W_conv2T.assign(self.W_conv2),self.b_conv2T.assign(self.b_conv2),self.W_conv3T.assign(self.W_conv3),self.b_conv3T.assign(self.b_conv3),self.W_fc1T.assign(self.W_fc1),self.b_fc1T.assign(self.b_fc1),self.W_fc2T.assign(self.W_fc2),self.b_fc2T.assign(self.b_fc2)]</span><br><span class="line"></span><br><span class="line">		self.createTrainingMethod()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># saving and loading networks</span></span><br><span class="line">		self.saver = tf.train.Saver()</span><br><span class="line">		self.session = tf.InteractiveSession()</span><br><span class="line">		self.session.run(tf.initialize_all_variables())</span><br><span class="line">		checkpoint = tf.train.get_checkpoint_state(<span class="string">&quot;saved_networks&quot;</span>)</span><br><span class="line">		<span class="keyword">if</span> checkpoint <span class="keyword">and</span> checkpoint.model_checkpoint_path:</span><br><span class="line">				self.saver.restore(self.session, checkpoint.model_checkpoint_path)</span><br><span class="line">				<span class="built_in">print</span> <span class="string">&quot;Successfully loaded:&quot;</span>, checkpoint.model_checkpoint_path</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">				<span class="built_in">print</span> <span class="string">&quot;Could not find old network weights&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">createQNetwork</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="comment"># network weights</span></span><br><span class="line">		W_conv1 = self.weight_variable([<span class="number">8</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">32</span>])</span><br><span class="line">		b_conv1 = self.bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">		W_conv2 = self.weight_variable([<span class="number">4</span>,<span class="number">4</span>,<span class="number">32</span>,<span class="number">64</span>])</span><br><span class="line">		b_conv2 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">		W_conv3 = self.weight_variable([<span class="number">3</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>])</span><br><span class="line">		b_conv3 = self.bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">		W_fc1 = self.weight_variable([<span class="number">1600</span>,<span class="number">512</span>])</span><br><span class="line">		b_fc1 = self.bias_variable([<span class="number">512</span>])</span><br><span class="line"></span><br><span class="line">		W_fc2 = self.weight_variable([<span class="number">512</span>,self.actions])</span><br><span class="line">		b_fc2 = self.bias_variable([self.actions])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># input layer</span></span><br><span class="line"></span><br><span class="line">		stateInput = tf.placeholder(<span class="string">&quot;float&quot;</span>,[<span class="literal">None</span>,<span class="number">80</span>,<span class="number">80</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># hidden layers</span></span><br><span class="line">		h_conv1 = tf.nn.relu(self.conv2d(stateInput,W_conv1,<span class="number">4</span>) + b_conv1)</span><br><span class="line">		h_pool1 = self.max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">		h_conv2 = tf.nn.relu(self.conv2d(h_pool1,W_conv2,<span class="number">2</span>) + b_conv2)</span><br><span class="line"></span><br><span class="line">		h_conv3 = tf.nn.relu(self.conv2d(h_conv2,W_conv3,<span class="number">1</span>) + b_conv3)</span><br><span class="line"></span><br><span class="line">		h_conv3_flat = tf.reshape(h_conv3,[-<span class="number">1</span>,<span class="number">1600</span>])</span><br><span class="line">		h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat,W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Q Value layer</span></span><br><span class="line">		QValue = tf.matmul(h_fc1,W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> stateInput,QValue,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">copyTargetQNetwork</span>(<span class="params">self</span>):</span><br><span class="line">		self.session.run(self.copyTargetQNetworkOperation)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">createTrainingMethod</span>(<span class="params">self</span>):</span><br><span class="line">		self.actionInput = tf.placeholder(<span class="string">&quot;float&quot;</span>,[<span class="literal">None</span>,self.actions])</span><br><span class="line">		self.yInput = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="literal">None</span>])</span><br><span class="line">		Q_Action = tf.reduce_sum(tf.mul(self.QValue, self.actionInput), reduction_indices = <span class="number">1</span>)</span><br><span class="line">		self.cost = tf.reduce_mean(tf.square(self.yInput - Q_Action))</span><br><span class="line">		self.trainStep = tf.train.AdamOptimizer(<span class="number">1e-6</span>).minimize(self.cost)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">trainQNetwork</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="comment"># Step 1: obtain random minibatch from replay memory</span></span><br><span class="line">		minibatch = random.sample(self.replayMemory,BATCH_SIZE)</span><br><span class="line">		state_batch = [data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">		action_batch = [data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">		reward_batch = [data[<span class="number">2</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line">		nextState_batch = [data[<span class="number">3</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Step 2: calculate y</span></span><br><span class="line">		y_batch = []</span><br><span class="line">		QValue_batch = self.QValueT.<span class="built_in">eval</span>(feed_dict=&#123;self.stateInputT:nextState_batch&#125;)</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,BATCH_SIZE):</span><br><span class="line">			terminal = minibatch[i][<span class="number">4</span>]</span><br><span class="line">			<span class="keyword">if</span> terminal:</span><br><span class="line">				y_batch.append(reward_batch[i])</span><br><span class="line">			<span class="keyword">else</span>:</span><br><span class="line">				y_batch.append(reward_batch[i] + GAMMA * np.<span class="built_in">max</span>(QValue_batch[i]))</span><br><span class="line"></span><br><span class="line">		self.trainStep.run(feed_dict=&#123;</span><br><span class="line">			self.yInput : y_batch,</span><br><span class="line">			self.actionInput : action_batch,</span><br><span class="line">			self.stateInput : state_batch</span><br><span class="line">			&#125;)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># save network every 100000 iteration</span></span><br><span class="line">		<span class="keyword">if</span> self.timeStep % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">			self.saver.save(self.session, <span class="string">&#x27;saved_networks/&#x27;</span> + <span class="string">&#x27;network&#x27;</span> + <span class="string">&#x27;-dqn&#x27;</span>, global_step = self.timeStep)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> self.timeStep % UPDATE_TIME == <span class="number">0</span>:</span><br><span class="line">			self.copyTargetQNetwork()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">setPerception</span>(<span class="params">self,nextObservation,action,reward,terminal</span>):</span><br><span class="line">		<span class="comment">#newState = np.append(nextObservation,self.currentState[:,:,1:],axis = 2)</span></span><br><span class="line">		newState = np.append(self.currentState[:,:,<span class="number">1</span>:],nextObservation,axis = <span class="number">2</span>)</span><br><span class="line">		self.replayMemory.append((self.currentState,action,reward,newState,terminal))</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">len</span>(self.replayMemory) &gt; REPLAY_MEMORY:</span><br><span class="line">			self.replayMemory.popleft()</span><br><span class="line">		<span class="keyword">if</span> self.timeStep &gt; OBSERVE:</span><br><span class="line">			<span class="comment"># Train the network</span></span><br><span class="line">			self.trainQNetwork()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># print info</span></span><br><span class="line">		state = <span class="string">&quot;&quot;</span></span><br><span class="line">		<span class="keyword">if</span> self.timeStep &lt;= OBSERVE:</span><br><span class="line">			state = <span class="string">&quot;observe&quot;</span></span><br><span class="line">		<span class="keyword">elif</span> self.timeStep &gt; OBSERVE <span class="keyword">and</span> self.timeStep &lt;= OBSERVE + EXPLORE:</span><br><span class="line">			state = <span class="string">&quot;explore&quot;</span></span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			state = <span class="string">&quot;train&quot;</span></span><br><span class="line"></span><br><span class="line">		<span class="built_in">print</span> <span class="string">&quot;TIMESTEP&quot;</span>, self.timeStep, <span class="string">&quot;/ STATE&quot;</span>, state, \</span><br><span class="line">            <span class="string">&quot;/ EPSILON&quot;</span>, self.epsilon</span><br><span class="line"></span><br><span class="line">		self.currentState = newState</span><br><span class="line">		self.timeStep += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">getAction</span>(<span class="params">self</span>):</span><br><span class="line">		QValue = self.QValue.<span class="built_in">eval</span>(feed_dict= &#123;self.stateInput:[self.currentState]&#125;)[<span class="number">0</span>]</span><br><span class="line">		action = np.zeros(self.actions)</span><br><span class="line">		action_index = <span class="number">0</span></span><br><span class="line">		<span class="keyword">if</span> self.timeStep % FRAME_PER_ACTION == <span class="number">0</span>:</span><br><span class="line">			<span class="keyword">if</span> random.random() &lt;= self.epsilon:</span><br><span class="line">				action_index = random.randrange(self.actions)</span><br><span class="line">				action[action_index] = <span class="number">1</span></span><br><span class="line">			<span class="keyword">else</span>:</span><br><span class="line">				action_index = np.argmax(QValue)</span><br><span class="line">				action[action_index] = <span class="number">1</span></span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			action[<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># do nothing</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># change episilon</span></span><br><span class="line">		<span class="keyword">if</span> self.epsilon &gt; FINAL_EPSILON <span class="keyword">and</span> self.timeStep &gt; OBSERVE:</span><br><span class="line">			self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/EXPLORE</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">setInitState</span>(<span class="params">self,observation</span>):</span><br><span class="line">		self.currentState = np.stack((observation, observation, observation, observation), axis = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">weight_variable</span>(<span class="params">self,shape</span>):</span><br><span class="line">		initial = tf.truncated_normal(shape, stddev = <span class="number">0.01</span>)</span><br><span class="line">		<span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">bias_variable</span>(<span class="params">self,shape</span>):</span><br><span class="line">		initial = tf.constant(<span class="number">0.01</span>, shape = shape)</span><br><span class="line">		<span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">self,x, W, stride</span>):</span><br><span class="line">		<span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, stride, stride, <span class="number">1</span>], padding = <span class="string">&quot;SAME&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">max_pool_2x2</span>(<span class="params">self,x</span>):</span><br><span class="line">		<span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">&quot;SAME&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>下面是使用上面的DQN让agent玩Gym的Breakout-v0游戏。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -------------------------</span></span><br><span class="line"><span class="comment"># Project: Deep Q-Learning on Breakout-v0</span></span><br><span class="line"><span class="comment"># Author: Houjiang Chen</span></span><br><span class="line"><span class="comment"># Date: 2017.4.25</span></span><br><span class="line"><span class="comment"># -------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> BrainDQN_Nature <span class="keyword">import</span> BrainDQN</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocess raw image to 80*80 gray image</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">observation</span>):</span><br><span class="line">    observation = cv2.cvtColor(cv2.resize(observation, (<span class="number">80</span>, <span class="number">80</span>)), cv2.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment">#ret, observation = cv2.threshold(observation, 1, 255, cv2.THRESH_BINARY)</span></span><br><span class="line">    <span class="keyword">return</span> np.reshape(observation, (<span class="number">80</span>, <span class="number">80</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">play</span>():</span><br><span class="line">    env = gym.make(<span class="string">&#x27;Breakout-v0&#x27;</span>)</span><br><span class="line">    actions = env.action_space.n</span><br><span class="line"></span><br><span class="line">    <span class="comment"># init BrainDQN</span></span><br><span class="line">    brain = BrainDQN(actions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        state = env.reset()</span><br><span class="line">        state = cv2.cvtColor(cv2.resize(state, (<span class="number">80</span>, <span class="number">80</span>)), cv2.COLOR_BGR2GRAY)</span><br><span class="line">        <span class="comment">#ret, state = cv2.threshold(state, 1, 255, cv2.THRESH_BINARY)</span></span><br><span class="line">        brain.setInitState(state)</span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            action = brain.getAction()</span><br><span class="line">            state, reward, terminal, _ = env.step(np.argmax(action))</span><br><span class="line">            env.render()</span><br><span class="line">            <span class="keyword">if</span> terminal:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = preprocess(state)</span><br><span class="line">            brain.setPerception(state, action, reward, terminal)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    play()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="参考资料">参考资料</h2>
<p>1、Reinforcement Learning: An Introduction, Richard S. Sutton and
Andrew G. Barto，2012<br />
2、Playing Atari with Deep Reinforcement Learning，DeepMind
Technologies，Arxiv 2013.12<br />
3、Human-level control through deep reinforcement learning，DeepMind
Technologies，Nature 2015.02<br />
4、DeepMind官网
https://deepmind.com/blog/deep-reinforcement-learning<br />
5、https://www.nervanasys.com/demystifying-deep-reinforcement-learning<br />
6、http://www.cnblogs.com/jinxulin/p/3511298.html<br />
7、Introduction to Reinforcement Learning，David Silver</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/reinforcement-learning/">reinforcement learning</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/reinforcement-learning/">reinforcement learning</a><a href="/tags/machine-learning/">machine learning</a>
  </div>

</div>



</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/06/21/ASGD中momentum的影响/" title="多节点异步更新中momentum的影响">
  <strong>上一篇：</strong><br/>
  <span>
  多节点异步更新中momentum的影响</span>
</a>
</div>


<div class="next">
<a href="/2017/04/10/值函数的贝尔曼公式推导/"  title="值函数的贝尔曼公式推导">
 <strong>下一篇：</strong><br/> 
 <span>值函数的贝尔曼公式推导
</span>
</a>
</div>

</nav>

	



</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#dqn"><span class="toc-number">1.</span> <span class="toc-text">DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#q-learning%E4%BE%8B%E5%AD%90"><span class="toc-number">1.1.</span> <span class="toc-text">Q learning例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91"><span class="toc-number">1.2.</span> <span class="toc-text">函数逼近</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dqn-1"><span class="toc-number">1.3.</span> <span class="toc-text">DQN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="toc-number">1.4.</span> <span class="toc-text">经验回放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.5.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gym%E4%BD%BF%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">Gym使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gym%E7%AE%80%E4%BB%8B"><span class="toc-number">2.1.</span> <span class="toc-text">Gym简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAgym%E7%8E%AF%E5%A2%83"><span class="toc-number">2.2.</span> <span class="toc-text">创建一个Gym环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#step"><span class="toc-number">2.3.</span> <span class="toc-text">step</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reset"><span class="toc-number">2.4.</span> <span class="toc-text">reset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#render"><span class="toc-number">2.5.</span> <span class="toc-text">render</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spaces"><span class="toc-number">2.6.</span> <span class="toc-text">Spaces</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#breakout-v0%E4%BE%8B%E5%AD%90"><span class="toc-number">2.7.</span> <span class="toc-text">Breakout-v0例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">3.</span> <span class="toc-text">参考资料</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  


  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/DL-Compiler/" title="DL Compiler">DL Compiler<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/Daily/" title="Daily">Daily<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/ML-framework/" title="ML framework">ML framework<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/XRT/" title="XRT">XRT<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/code/" title="code">code<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/deep-learning/" title="deep learning">deep learning<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/graph-optimization-图优化/" title="graph optimization, 图优化">graph optimization, 图优化<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/kaldi-decision-tree-决策树/" title="kaldi, decision tree, 决策树">kaldi, decision tree, 决策树<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/low-bitwidth/" title="low bitwidth">low bitwidth<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/model-compression/" title="model compression">model compression<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/neural-machine-translation/" title="neural machine translation">neural machine translation<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/tvm-knowledge/" title="tvm knowledge">tvm knowledge<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Deep-Learning-Compiler/" title="Deep Learning Compiler">Deep Learning Compiler<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/IREE/" title="IREE">IREE<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/caffe/" title="caffe">caffe<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/deep-learning/" title="deep learning">deep learning<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/machine-learning/" title="machine learning">machine learning<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/momentum/" title="momentum">momentum<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/XLA/" title="XLA">XLA<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/FusionStitching/" title="FusionStitching">FusionStitching<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/c/" title="c++">c++<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/embedding/" title="embedding">embedding<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/kaldi/" title="kaldi">kaldi<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/decision-tree/" title="decision tree">decision tree<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/决策树/" title="决策树">决策树<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/HMM/" title="HMM">HMM<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/上下文相关音素/" title="上下文相关音素">上下文相关音素<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/web-technology/" title="web technology">web technology<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/TVM/" title="TVM">TVM<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/PackedFunc/" title="PackedFunc">PackedFunc<sup>1</sup></a></li>
			
		
		</ul>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2023 
		
		<a href="/about" target="_blank" title="Dou Jiang">Dou Jiang</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
