<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hjchen2.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据&#x2F;模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。 原始论文 KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba">
<meta property="og:type" content="article">
<meta property="og:title" content="阿里KunPeng框架学习">
<meta property="og:url" content="https://hjchen2.github.io/2017/08/22/KunPeng%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.html">
<meta property="og:site_name" content="Don&#39;t Respond">
<meta property="og:description" content="KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据&#x2F;模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。 原始论文 KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba">
<meta property="og:locale">
<meta property="article:published_time" content="2017-08-22T04:53:08.000Z">
<meta property="article:modified_time" content="2023-01-03T14:04:48.173Z">
<meta property="article:author" content="Dou Jiang">
<meta property="article:tag" content="large scale ML framework">
<meta property="article:tag" content="KunPeng">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://hjchen2.github.io/2017/08/22/KunPeng%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"https://hjchen2.github.io/2017/08/22/KunPeng%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","path":"2017/08/22/KunPeng论文阅读/","title":"阿里KunPeng框架学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>阿里KunPeng框架学习 | Don't Respond</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Don't Respond</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">Home</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">Tags</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kunpeng%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">KunPeng整体架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#apsara-cloud-platform"><span class="nav-number">2.1.</span> <span class="nav-text">Apsara Cloud Platform</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kunpeng-platform"><span class="nav-number">3.</span> <span class="nav-text">KunPeng Platform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E9%94%99%E6%96%B9%E6%A1%88"><span class="nav-number">3.1.</span> <span class="nav-text">容错方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dag%E8%B0%83%E5%BA%A6"><span class="nav-number">3.2.</span> <span class="nav-text">DAG调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E9%80%9A%E4%BF%A1%E6%8E%A5%E5%8F%A3"><span class="nav-number">3.3.</span> <span class="nav-text">负载均衡和通信接口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ftrl"><span class="nav-number">4.</span> <span class="nav-text">FTRL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mart"><span class="nav-number">5.</span> <span class="nav-text">MART</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95"><span class="nav-number">6.</span> <span class="nav-text">其他算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">7.</span> <span class="nav-text">实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#kunpengspark%E5%92%8Cmpi%E7%9A%84lr%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="nav-number">7.1.</span> <span class="nav-text">KunPeng、Spark和MPI的LR算法对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kunpeng-mart%E5%92%8Cxgboost%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">7.2.</span> <span class="nav-text">KunPeng-MART和XGBoost的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kunpeng-fmlibfm%E5%92%8Cdifacto%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">7.3.</span> <span class="nav-text">KunPeng-FM、LibFM和DiFacto的对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">8.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Dou Jiang"
      src="https://github.com/hjchen2/personal/blob/master/images/ETO_Logo%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5_%E5%90%91%E9%87%91%E5%AD%97%E5%A1%94%E9%AB%98%E4%B8%BE%E5%8F%8C%E8%87%82.jpg?raw=true">
  <p class="site-author-name" itemprop="name">Dou Jiang</p>
  <div class="site-description" itemprop="description">凌绝顶，众山小</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hjchen2" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hjchen2" rel="noopener me" target="_blank">GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://hjchen2.github.io/2017/08/22/KunPeng%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://github.com/hjchen2/personal/blob/master/images/ETO_Logo%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5_%E5%90%91%E9%87%91%E5%AD%97%E5%A1%94%E9%AB%98%E4%B8%BE%E5%8F%8C%E8%87%82.jpg?raw=true">
      <meta itemprop="name" content="Dou Jiang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Don't Respond">
      <meta itemprop="description" content="凌绝顶，众山小">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="阿里KunPeng框架学习 | Don't Respond">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          阿里KunPeng框架学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2017-08-22 12:53:08" itemprop="dateCreated datePublished" datetime="2017-08-22T12:53:08+08:00">2017-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-01-03 22:04:48" itemprop="dateModified" datetime="2023-01-03T22:04:48+08:00">2023-01-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML-framework/" itemprop="url" rel="index"><span itemprop="name">ML framework</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据/模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。
原始论文 KunPeng: Parameter Server based Distributed Learning Systems
and Its Applications in Alibaba and Ant Financial。</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>主要对一些通用分布式计算框架进行比较。</p>
<p>Hadoop：只提供了一些粗粒度的操作，比如Map、Reduce和Join等。很多限制导致基于Hadoop的机器学习算法效率都非常低，这些限制包括中间结果会落盘、只能在shuffling阶段进行数据交换等。</p>
<p>Spark：使用RDD弥补了Hadoop的一些缺点，提供MLlib库，MLlib整合了很多机器学习算法，并且非常容易使用。但MLlib只支持中等规模的特征，计算和通信效率都比较低。一些公司使用第三方组件来弥补Spark的缺陷，但至今没有一个完美的方案。</p>
<p>GraphLab和GraphX：基于图的并行计算框架，允许用户进行细粒度的控制，但并不适合通用的机器学习算法，比如LR、深度学习等，并且也存在效率低的问题。</p>
<p>MPI：接口灵活高效，代码自由度比较高，比如在代码中所有进程之间可以随时通信。但使用MPI开发一个新算法的开销非常大，比如一个复杂的异步矩阵分解算法需要2000多行代码。MPI没有提供分布式ML平台通用的组件，比如分布式数据读取，内存管理和多线程并行的组件。更重要的是MPI没有提供单点失败的本地解决方案，根据他们的统计数据显示MPI作业在节点数越多时失败率越高。</p>
<p>parameter
server框架：包含无状态的workers和有状态的servers，workers负责大部分的计算任务，servers负责保存和更新模型参数。servers可以定期将模型参数快照保存到一个缓存位置，一旦有节点失败，parameter
server会自动从最新的checkpoint中恢复模型参数。parameter
server框架只支持pserver和worker之间通信，
而pserver和pserver、worker和worker之间无法进行点对点通信，并且由于细粒度的接口导致用户编程比较复杂，因此现有的parameter
server框架还存在几个问题：一是通信接口比较单一，没有MPI灵活；二是对于用户来说没有Spark易于编程使用。</p>
<p>正是由于上述框架的种种缺点，他们开发了一个产品级的分布式学习系统—KunPeng。KunPeng结合了parameter
server和MPI的优点，提供鲁棒的failover机制，高效的稀疏数据通信接口和与MPI类似的通用接口，并且提供一个C++和Python的SDK，该SDK提供了一个类似单机的开发环境。KunPeng也与阿里的Apsara平台深度对接，提供ML的全工具集，包括基于SQL和MapReduce的数据预处理、预测、评估等等。</p>
<h2 id="kunpeng整体架构">KunPeng整体架构</h2>
<h3 id="apsara-cloud-platform">Apsara Cloud Platform</h3>
<p>Apsara是阿里开发的一个大规模分布式操作系统，目前已运行在跨数十个机房的十几万台服务器上。下图中天蓝色部分就是Apsara的模块，白色部分为运行在Apsara之上的各种云服务，KunPeng就属于图中白色部分，运行在Apsara上，由Apsara提供任务调度和监控、文件系统等服务。
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/b2b0cb8a6973ec2b4281d68c328e4a0f.png"
alt="b2b0cb8a6973ec2b4281d68c328e4a0f" />
图中红色边框的任务调度模块和资源管理模块被统称为Fuxi（伏羲），Fuxi支持多种特性以保证系统的可扩展性和容错性，这些特性包括：增量资源管理协议、用户透明的失败恢复、故障点自动检测和多级黑名单机制。</p>
<h2 id="kunpeng-platform">KunPeng Platform</h2>
<p>KunPeng分为ML-Bridge和PS-Core两个子系统，ML-Bridge是KunPeng提供的高级编程模型，用户通过脚本编程的workflow可以方便地实现数据预处理、训练、预测和评估等算法，PS-Core是一个分布式键值对存储的paramter
server框架。 <img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/0313b564c3646a4c4fab16574f9c4b4e.png?api=v2%20=600"
alt="0313b564c3646a4c4fab16574f9c4b4e" /> ML-Bridge由三个组件构成：</p>
<ul>
<li>解释器。将用户的脚本解释为系统支持的算法</li>
<li>优化器。根据运行状态的历史统计和启发式方法，分析、调试和优化作业配置</li>
<li>执行器。根据作业的配置生成Fuxi调度的配置，提供整个作业生命周期的监控，并提供用户监控UI
ML-Bridge简化了用户编程，比如一个算法流程包括数据入库与预处理、训练、评估和AB测试几个流程，在KunPeng中只需要调用下图中的几行命令就可以实现。整个流程对用户来说都是透明的，用户也不需要关心算法的具体实现和作业调度过程。</li>
</ul>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/ede2df215585fc86358bc9868565d1ce.png?api=v2%20=500"
alt="ede2df215585fc86358bc9868565d1ce" />
<figcaption
aria-hidden="true">ede2df215585fc86358bc9868565d1ce</figcaption>
</figure>
<p>PS-Core不仅支持数据并行和模型并行，同时还支持模型同步更新(BSP)、ASP和SSP，稀疏表达和容错机制。
PS-Core在传统的worker和server基础上，增加了一个用于迭代控制的coordinator。coordinator声明了数据计算和参数更新的操作，构建了整个ML
workerflows的作业图，并将这些作业调度到worker和server上运行，并参与servers和workers的failover过程。coordinator在迭代结束时会与Apsara的meta对迭代状态进行同步，并且由Fuxi监控管理，因此不存在SPOF（单点失败）的问题。</p>
<h3 id="容错方案">容错方案</h3>
<p>KunPeng也给出了servers和workers的容错解决方案。对于servers，它们会异步地将参数快照保存到分布式文件系统，并且它们会在内存中对参数进行两备份，支持hot
failover加速恢复过程。大多数情况下(比如接收到coordinator的恢复请求)，servers可以立刻通过内存备份的参数中恢复。即使是servers或整个任务被中断或被kill，servers也可以通过最近一次保存的参数进行恢复训练。对于stateless的workers，failover非常简单，只需要从servers上pull对应的参数。对于stateful的workers，同样提供保存快照的接口，因此对于一些workers有本地状态的算法（比如LDA），faliover也非常简单。</p>
<p>总的来说，KunPeng的failover过程是当Fuxi检测到有节点失败时，重新调度新的节点，同时给coordinator发送异步节点失败的消息，coordinator接收消息后给servers和workers发送恢复请求，对于正常的servers接收请求后会直接从内存中恢复，而对于新调度的servers会从checkpoint中恢复，对于workers需要先从servers上pull对应的参数，stateful的workers还需要从保存的checkpoint中恢复状态。</p>
<h3 id="dag调度">DAG调度</h3>
<p>这里的调度指的是coordinator对servers和workers的调度。由于coordinator节点会根据算法的workflow构建对应的作业DAG，并将DAG调度到servers和workers上进行执行。为了提高机器资源利用率和作业效率，DAG中相同深度的节点可以并行执行，比如下图中的Calculate
for Block 0节点和Load Data for Block
1节点。通过DAG接口用户可以自定义IO操作、计算和通信过程，可以很方便地实现各种模型更新算法。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/e76cf7c13015b83ed7696b5fa7c8dac0.png?api=v2%20=600"
alt="e76cf7c13015b83ed7696b5fa7c8dac0" />
<figcaption
aria-hidden="true">e76cf7c13015b83ed7696b5fa7c8dac0</figcaption>
</figure>
<p>下图表示了PS-Core中bounded delay
ASGD算法的C++实现，用户可以重写下面的Iterate函数实现自定义的算法。图中的mServerParam和mServerGrad对应servers上的模型参数和梯度，mWorkerParam和mWorkerGrad对应workers本地的模型参数和梯度，mSubDatasetPtr对应当前worker的数据子集。nSync为最大延迟迭代次数，nPull和nPush分别为从servers获取最新参数和将梯度发送给servers的频率。通过设置nSync、nPull和nPush可以很方便地在BSP和SSP之间切换，而去除SyncBarrier就成了ASP算法的实现。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/69ed0d3573fbebf558494bc4a9a14c74.png?api=v2%20=450"
alt="69ed0d3573fbebf558494bc4a9a14c74" />
<figcaption
aria-hidden="true">69ed0d3573fbebf558494bc4a9a14c74</figcaption>
</figure>
<h3 id="负载均衡和通信接口">负载均衡和通信接口</h3>
<p>由于集群中机器的底层硬件和运行状态存在差异，因此一个任务的执行效率很大程度上取决于运行最慢的那个机器，针对这种情况可以有多种负载均衡的方法，比如可以对负载较高的机器分配更少的数据和计算量，PS-Core也为此设计了一个Backup
instance机制。当某个节点被确定为慢节点时，coordinator会把慢节点标记为"dead"节点，请求Fuxi重新调度一个新的节点作为该节点的备份节点，并将该节点的负载转移到备份节点上。这种机制通常可以带来10%-20%的效率提升。</p>
<p>KunPeng对不同稀疏度和不同数据类型的数据通信做了深度优化，并且提供workers之间点对点的通信接口，比如AllReduce，ReduceTo和Bcast，这些灵活的通信接口使得KunPeng可以拓展更多的功能，比如模型并行。</p>
<h2 id="ftrl">FTRL</h2>
<p><span
class="math display">\[w_{t+1}=\mathop{\arg\min}_{w}\left(\sum_{s=1}^{t}g_{s}w+\frac{1}{2}\sum_{s=1}^{t}\delta_{s}{\Vert}w-w_{s}{\Vert}_{2}^{2}+\lambda_{1}{\Vert}w{\Vert}_{1}+\lambda_{2}{\Vert}w{\Vert}_{2}^{2}\right)\]</span>
其中<span class="math inline">\(g\)</span>为损失函数对<span
class="math inline">\(w\)</span>的梯度，<span
class="math inline">\(\delta_{t}=\frac{1}{\eta_{t}}-\frac{1}{\eta_{t-1}}\)</span>，因此<span
class="math inline">\(\sum_{s=1}^{t}{\delta_{s}}=\frac{1}{\eta_{t}}\)</span>，<span
class="math inline">\(\eta\)</span>为学习率，并且<span
class="math inline">\(\eta_{t,i}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^{s}{g_{s,i}^2}}}\)</span>，通常<span
class="math inline">\(\alpha=1\)</span>，<span
class="math inline">\(\beta\)</span>是与数据集和特征相关的超参数。<span
class="math inline">\(\lambda_{1}\)</span>为L1系数，<span
class="math inline">\(\lambda_{2}\)</span>为L2系数。 更新公式为<br />
<span class="math display">\[w_{t+1}=\begin{cases}0&amp; if\
{\vert}z_{i}{\vert}{\leq}\lambda_{1}\\
-(\frac{\beta+\sqrt{n_{i}}}{\alpha}+\lambda_{2})^{-1}(z_{i}-sign(z_{i})\lambda_{1})&amp;
otherwise\end{cases}\]</span> 下图表明了LR
FTRL-Proximal算法单机更新过程。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/66cf72a181547ae24831af8500b47d72.png?api=v2%20=500"
alt="66cf72a181547ae24831af8500b47d72" />
<figcaption
aria-hidden="true">66cf72a181547ae24831af8500b47d72</figcaption>
</figure>
<p>这个算法在单机时很容易实现，但在分布式环境必须要考虑通信效率、servers的负载和算法收敛性问题。考虑到BSP的低效和ASP可能不收敛的问题，他们使用了bounded
delay的SSP更新方法，并且设置trust
region来调节参数范围，避免模型发散。整个算法具体过程如下：</p>
<ul>
<li>workers本地保存了模型<span class="math inline">\(w\)</span>和<span
class="math inline">\(z\)</span>、<span
class="math inline">\(n\)</span>，<span
class="math inline">\(z\)</span>、<span
class="math inline">\(n\)</span>通过bounded-asynchronous的方式与servers保持同步</li>
<li>workers加载数据，根据<span class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>更新本地模型<span
class="math inline">\(w\)</span>，计算梯度并更新本地模型<span
class="math inline">\(w\)</span>和<span
class="math inline">\(z\)</span>、<span
class="math inline">\(n\)</span>，同时使用<span
class="math inline">\(\delta_{z}\)</span>和<span
class="math inline">\(\delta_{n}\)</span>累加<span
class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>的增量，在需要与servers同步的时候将累加的<span
class="math inline">\(\delta_{z}\)</span>和<span
class="math inline">\(\delta_{n}\)</span> push到servers</li>
<li>servers合并所有workers发送的<span
class="math inline">\(\delta_{z}\)</span>和<span
class="math inline">\(\delta_{n}\)</span>，最后更新全局<span
class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>。</li>
</ul>
<p>workers向servers传递<span class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>的增量，而不是直接传递模型梯度，这种做法虽然会带来一些通信开销，但降低了servers的计算负载，这是在通信效率和计算负载之间做的平衡。为了避免发散，servers在trust
region下更新模型。trust
region的策略有两种：一种是当模型中的元素超出置信阈时，直接回退整个模型；另一种是通过映射的方式将模型的值限制在置信阈中。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/0de2241d38a792bb79446944d65d8c66.png?api=v2%20=600"
alt="0de2241d38a792bb79446944d65d8c66" />
<figcaption
aria-hidden="true">0de2241d38a792bb79446944d65d8c66</figcaption>
</figure>
<h2 id="mart">MART</h2>
<p>MART（多增量回归树）又叫做GBDT，是一种应用比较广泛的机器学习算法。KunPeng实现了一个通用的MART算法，支持千亿级样本量和上千维的特征，并在MART的基础上实现了LambdaMART算法。</p>
<ul>
<li>MART
为了处理超大规模的数据量，KunPeng-MART使用数据并行的方式减少内存使用量，并采用了XGBoost的分布式加权直方图算法优化分裂点查找过程。具体来说就是，每个worker都保存了整颗树，在分割叶节点时，
（1）每个worker使用分配的数据子集计算一个局部加权直方图，计算完成后将直方图push到servers
（2）servers收到workers发送的直方图后，采用多路合并算法得到全局直方图，并找到最优分割点
（3）workers从servers
pull分割点，分裂节点并将数据分到分裂后的叶节点</li>
</ul>
<p>重复上述过程，可以得到整棵树。然后只要按照gradient
boosting方法一棵一棵地建树，最终得到MART。随着特征维度和树深度的增加，查找分裂点过程中的计算和通信都可能成为性能瓶颈。为了解决这个问题，他们提到使用KunPeng的通信模式去减少合并局部直方图的开销，但并没有透露具体的方法。</p>
<ul>
<li>LambdaMART
LambdaMART建树的过程与上面的MART一样，不同的是LambdaMART计算一阶导数和二阶导数的方式。由于LambdaMART要求同一个query
group的训练数据按sample两两组成pair对，因此当训练数据不是按照query
group连续存储时就会存在问题。对于这个问题，他们提出了两种解决方法：<br />
（1）先全局统计一下每个query id对应的样本总数，然后按照multiway number
partitioning algorithm对query
id进行分片，每个worker只加载属于自己的query ids对应的训练样本。<br />
（2）第二种是近似的方法。首先要求相同query
id的样本在文件系统中是连续存储的，然后每个worker还是按照正常情况加载属于自己的分片数据。如果相同query
id的样本被分在两个不同的worker上，则会把这两个worker上相同query
id的样本当做不同query id来处理。</li>
</ul>
<h2 id="其他算法">其他算法</h2>
<ul>
<li>Large-scale sparse Logistic Regression (LR)<br />
实现了不同的优化算法，L-BFGS、OWL-QN和BCD，其中BCD算法是数据和模型同时并行的算法。<br />
</li>
<li>Distributed Factorization Machines<br />
workers异步计算梯度，使用AdaGrad优化算法<br />
</li>
<li>Caffe<br />
实现了Caffe和KunPeng的对接，a generalized CPU-based large-scale deep
learning platform，简化DL算法开发</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<p>下面的实验都是在一个拥有5000台服务器的正式集群上进行的，每台机器12个Intel
Xeon CPU E5-2430 (2.2 GHz) CPU和96GB内存。</p>
<h3
id="kunpengspark和mpi的lr算法对比">KunPeng、Spark和MPI的LR算法对比</h3>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/143e082b7f1a6b54e47e9c8b51026dbb.png?api=v2"
alt="143e082b7f1a6b54e47e9c8b51026dbb" />
<figcaption
aria-hidden="true">143e082b7f1a6b54e47e9c8b51026dbb</figcaption>
</figure>
<p>不同平台的LR都采用L-BFGS算法更新，并且memory history
parameter都设置为10，并且使用同一个集群相同的CPU资源，在7个不同的数据集上KunPeng在效率和内存占用上都取得非常明显的优势。</p>
<p>在另外一个18 billion样本和 7
billion特征的数据集上，他们统计了KunPeng在不同workers数量时的加速比。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/00c84f368394ba04d59dbe530f69c387.png?api=v2"
alt="00c84f368394ba04d59dbe530f69c387" />
<figcaption
aria-hidden="true">00c84f368394ba04d59dbe530f69c387</figcaption>
</figure>
<p>KunPeng仅使用25个workers就可以训练这么大的数据，workers增加时依然能保持较高的加速比，并且内存占用随着workers增加而近乎直线降低。</p>
<h3 id="kunpeng-mart和xgboost的对比">KunPeng-MART和XGBoost的对比</h3>
<p>下图分别为KunPeng-MAR和XGBoost在不同任务上的峰值内存占用和训练时间对比。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/1b0888cab293242eaccdc2b6e5bf25d9.png?api=v2%20=500"
alt="1b0888cab293242eaccdc2b6e5bf25d9" />
<figcaption
aria-hidden="true">1b0888cab293242eaccdc2b6e5bf25d9</figcaption>
</figure>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/3b99dc82bc268d3da394a688c0234908.png?api=v2%20=500"
alt="3b99dc82bc268d3da394a688c0234908" />
<figcaption
aria-hidden="true">3b99dc82bc268d3da394a688c0234908</figcaption>
</figure>
<h3
id="kunpeng-fmlibfm和difacto的对比">KunPeng-FM、LibFM和DiFacto的对比</h3>
<p>下面是在单机情况下的训练效果对比，并没有训练时间的对比数据和多机实验相关的数据。</p>
<figure>
<img
src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/da511a1bb0db987fb74ebb08fa5352c9.png?api=v2%20=500"
alt="da511a1bb0db987fb74ebb08fa5352c9" />
<figcaption
aria-hidden="true">da511a1bb0db987fb74ebb08fa5352c9</figcaption>
</figure>
<h2 id="参考资料">参考资料</h2>
<p>1、Ad Click Prediction: a View from the Trenches.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/large-scale-ML-framework/" rel="tag"># large scale ML framework</a>
              <a href="/tags/KunPeng/" rel="tag"># KunPeng</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2017/07/03/C++%E8%B0%83%E7%94%A8Python%E6%8E%A5%E5%8F%A3/" rel="prev" title="C++调用python">
                  <i class="fa fa-chevron-left"></i> C++调用python
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2017/12/01/seq2seq%E4%B8%B2%E8%AE%B2/" rel="next" title="NEURAL MACHINE TRANSLATION论文学习串讲">
                  NEURAL MACHINE TRANSLATION论文学习串讲 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dou Jiang</span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"hjchen2","repo":"hjchen2.github.io","client_id":"5248e75f54358b4eded8","client_secret":"d8b8ae05a0d4625d8ac01e4ab167f88cd0e0f2b1","admin_user":"hjchen2","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"1bb2f2d67398787ffad1042998cb7984"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
