
 <!DOCTYPE HTML>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
  
    <title>阿里KunPeng框架学习 | Don&#39;t Respond</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Dou Jiang">
    

    
    <meta name="description" content="KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据&#x2F;模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。 原始论文 KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba">
<meta property="og:type" content="article">
<meta property="og:title" content="阿里KunPeng框架学习">
<meta property="og:url" content="https://hjchen2.github.io/2017/08/22/KunPeng%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.html">
<meta property="og:site_name" content="Don&#39;t Respond">
<meta property="og:description" content="KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据&#x2F;模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。 原始论文 KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba">
<meta property="og:locale">
<meta property="article:published_time" content="2017-08-22T04:53:08.000Z">
<meta property="article:modified_time" content="2023-02-07T02:49:01.177Z">
<meta property="article:author" content="Dou Jiang">
<meta property="article:tag" content="large scale ML framework">
<meta property="article:tag" content="KunPeng">
<meta name="twitter:card" content="summary">

    
    <link rel="alternative" href="/atom.xml" title="Don&#39;t Respond" type="application/atom+xml">
    
    
    
    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

  <body>
    <header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Don&#39;t Respond">Don&#39;t Respond</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:hjchen2.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/08/22/KunPeng论文阅读/" title="阿里KunPeng框架学习" itemprop="url">阿里KunPeng框架学习</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Dou Jiang" target="_blank" itemprop="author">Dou Jiang</a>
		
  <p class="article-time">
    <time datetime="2017-08-22T04:53:08.000Z" itemprop="datePublished"> Published 2017-08-22</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kunpeng%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">KunPeng整体架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#apsara-cloud-platform"><span class="toc-number">2.1.</span> <span class="toc-text">Apsara Cloud Platform</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kunpeng-platform"><span class="toc-number">3.</span> <span class="toc-text">KunPeng Platform</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E6%96%B9%E6%A1%88"><span class="toc-number">3.1.</span> <span class="toc-text">容错方案</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dag%E8%B0%83%E5%BA%A6"><span class="toc-number">3.2.</span> <span class="toc-text">DAG调度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E9%80%9A%E4%BF%A1%E6%8E%A5%E5%8F%A3"><span class="toc-number">3.3.</span> <span class="toc-text">负载均衡和通信接口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ftrl"><span class="toc-number">4.</span> <span class="toc-text">FTRL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mart"><span class="toc-number">5.</span> <span class="toc-text">MART</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">其他算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">7.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kunpengspark%E5%92%8Cmpi%E7%9A%84lr%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">7.1.</span> <span class="toc-text">KunPeng、Spark和MPI的LR算法对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kunpeng-mart%E5%92%8Cxgboost%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">7.2.</span> <span class="toc-text">KunPeng-MART和XGBoost的对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kunpeng-fmlibfm%E5%92%8Cdifacto%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">7.3.</span> <span class="toc-text">KunPeng-FM、LibFM和DiFacto的对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">8.</span> <span class="toc-text">参考资料</span></a></li></ol>
		
		</div>
		
		<p>KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据/模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。
原始论文 KunPeng: Parameter Server based Distributed Learning Systems
and Its Applications in Alibaba and Ant Financial。</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>主要对一些通用分布式计算框架进行比较。</p>
<p>Hadoop：只提供了一些粗粒度的操作，比如Map、Reduce和Join等。很多限制导致基于Hadoop的机器学习算法效率都非常低，这些限制包括中间结果会落盘、只能在shuffling阶段进行数据交换等。</p>
<p>Spark：使用RDD弥补了Hadoop的一些缺点，提供MLlib库，MLlib整合了很多机器学习算法，并且非常容易使用。但MLlib只支持中等规模的特征，计算和通信效率都比较低。一些公司使用第三方组件来弥补Spark的缺陷，但至今没有一个完美的方案。</p>
<p>GraphLab和GraphX：基于图的并行计算框架，允许用户进行细粒度的控制，但并不适合通用的机器学习算法，比如LR、深度学习等，并且也存在效率低的问题。</p>
<p>MPI：接口灵活高效，代码自由度比较高，比如在代码中所有进程之间可以随时通信。但使用MPI开发一个新算法的开销非常大，比如一个复杂的异步矩阵分解算法需要2000多行代码。MPI没有提供分布式ML平台通用的组件，比如分布式数据读取，内存管理和多线程并行的组件。更重要的是MPI没有提供单点失败的本地解决方案，根据他们的统计数据显示MPI作业在节点数越多时失败率越高。</p>
<p>parameter
server框架：包含无状态的workers和有状态的servers，workers负责大部分的计算任务，servers负责保存和更新模型参数。servers可以定期将模型参数快照保存到一个缓存位置，一旦有节点失败，parameter
server会自动从最新的checkpoint中恢复模型参数。parameter
server框架只支持pserver和worker之间通信，
而pserver和pserver、worker和worker之间无法进行点对点通信，并且由于细粒度的接口导致用户编程比较复杂，因此现有的parameter
server框架还存在几个问题：一是通信接口比较单一，没有MPI灵活；二是对于用户来说没有Spark易于编程使用。</p>
<p>正是由于上述框架的种种缺点，他们开发了一个产品级的分布式学习系统—KunPeng。KunPeng结合了parameter
server和MPI的优点，提供鲁棒的failover机制，高效的稀疏数据通信接口和与MPI类似的通用接口，并且提供一个C++和Python的SDK，该SDK提供了一个类似单机的开发环境。KunPeng也与阿里的Apsara平台深度对接，提供ML的全工具集，包括基于SQL和MapReduce的数据预处理、预测、评估等等。</p>
<h2 id="kunpeng整体架构">KunPeng整体架构</h2>
<h3 id="apsara-cloud-platform">Apsara Cloud Platform</h3>
<p>Apsara是阿里开发的一个大规模分布式操作系统，目前已运行在跨数十个机房的十几万台服务器上。下图中天蓝色部分就是Apsara的模块，白色部分为运行在Apsara之上的各种云服务，KunPeng就属于图中白色部分，运行在Apsara上，由Apsara提供任务调度和监控、文件系统等服务。
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/b2b0cb8a6973ec2b4281d68c328e4a0f.png?raw=true"
alt="b2b0cb8a6973ec2b4281d68c328e4a0f" />
图中红色边框的任务调度模块和资源管理模块被统称为Fuxi（伏羲），Fuxi支持多种特性以保证系统的可扩展性和容错性，这些特性包括：增量资源管理协议、用户透明的失败恢复、故障点自动检测和多级黑名单机制。</p>
<h2 id="kunpeng-platform">KunPeng Platform</h2>
<p>KunPeng分为ML-Bridge和PS-Core两个子系统，ML-Bridge是KunPeng提供的高级编程模型，用户通过脚本编程的workflow可以方便地实现数据预处理、训练、预测和评估等算法，PS-Core是一个分布式键值对存储的paramter
server框架。 <img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/0313b564c3646a4c4fab16574f9c4b4e.png?raw=true%20=600"
alt="0313b564c3646a4c4fab16574f9c4b4e" /> ML-Bridge由三个组件构成：</p>
<ul>
<li>解释器。将用户的脚本解释为系统支持的算法</li>
<li>优化器。根据运行状态的历史统计和启发式方法，分析、调试和优化作业配置</li>
<li>执行器。根据作业的配置生成Fuxi调度的配置，提供整个作业生命周期的监控，并提供用户监控UI
ML-Bridge简化了用户编程，比如一个算法流程包括数据入库与预处理、训练、评估和AB测试几个流程，在KunPeng中只需要调用下图中的几行命令就可以实现。整个流程对用户来说都是透明的，用户也不需要关心算法的具体实现和作业调度过程。</li>
</ul>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/ede2df215585fc86358bc9868565d1ce.png?raw=true%20=500"
alt="ede2df215585fc86358bc9868565d1ce" />
<figcaption
aria-hidden="true">ede2df215585fc86358bc9868565d1ce</figcaption>
</figure>
<p>PS-Core不仅支持数据并行和模型并行，同时还支持模型同步更新(BSP)、ASP和SSP，稀疏表达和容错机制。
PS-Core在传统的worker和server基础上，增加了一个用于迭代控制的coordinator。coordinator声明了数据计算和参数更新的操作，构建了整个ML
workerflows的作业图，并将这些作业调度到worker和server上运行，并参与servers和workers的failover过程。coordinator在迭代结束时会与Apsara的meta对迭代状态进行同步，并且由Fuxi监控管理，因此不存在SPOF（单点失败）的问题。</p>
<h3 id="容错方案">容错方案</h3>
<p>KunPeng也给出了servers和workers的容错解决方案。对于servers，它们会异步地将参数快照保存到分布式文件系统，并且它们会在内存中对参数进行两备份，支持hot
failover加速恢复过程。大多数情况下(比如接收到coordinator的恢复请求)，servers可以立刻通过内存备份的参数中恢复。即使是servers或整个任务被中断或被kill，servers也可以通过最近一次保存的参数进行恢复训练。对于stateless的workers，failover非常简单，只需要从servers上pull对应的参数。对于stateful的workers，同样提供保存快照的接口，因此对于一些workers有本地状态的算法（比如LDA），faliover也非常简单。</p>
<p>总的来说，KunPeng的failover过程是当Fuxi检测到有节点失败时，重新调度新的节点，同时给coordinator发送异步节点失败的消息，coordinator接收消息后给servers和workers发送恢复请求，对于正常的servers接收请求后会直接从内存中恢复，而对于新调度的servers会从checkpoint中恢复，对于workers需要先从servers上pull对应的参数，stateful的workers还需要从保存的checkpoint中恢复状态。</p>
<h3 id="dag调度">DAG调度</h3>
<p>这里的调度指的是coordinator对servers和workers的调度。由于coordinator节点会根据算法的workflow构建对应的作业DAG，并将DAG调度到servers和workers上进行执行。为了提高机器资源利用率和作业效率，DAG中相同深度的节点可以并行执行，比如下图中的Calculate
for Block 0节点和Load Data for Block
1节点。通过DAG接口用户可以自定义IO操作、计算和通信过程，可以很方便地实现各种模型更新算法。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/e76cf7c13015b83ed7696b5fa7c8dac0.png?raw=true%20=600"
alt="e76cf7c13015b83ed7696b5fa7c8dac0" />
<figcaption
aria-hidden="true">e76cf7c13015b83ed7696b5fa7c8dac0</figcaption>
</figure>
<p>下图表示了PS-Core中bounded delay
ASGD算法的C++实现，用户可以重写下面的Iterate函数实现自定义的算法。图中的mServerParam和mServerGrad对应servers上的模型参数和梯度，mWorkerParam和mWorkerGrad对应workers本地的模型参数和梯度，mSubDatasetPtr对应当前worker的数据子集。nSync为最大延迟迭代次数，nPull和nPush分别为从servers获取最新参数和将梯度发送给servers的频率。通过设置nSync、nPull和nPush可以很方便地在BSP和SSP之间切换，而去除SyncBarrier就成了ASP算法的实现。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/69ed0d3573fbebf558494bc4a9a14c74.png?raw=true%20=450"
alt="69ed0d3573fbebf558494bc4a9a14c74" />
<figcaption
aria-hidden="true">69ed0d3573fbebf558494bc4a9a14c74</figcaption>
</figure>
<h3 id="负载均衡和通信接口">负载均衡和通信接口</h3>
<p>由于集群中机器的底层硬件和运行状态存在差异，因此一个任务的执行效率很大程度上取决于运行最慢的那个机器，针对这种情况可以有多种负载均衡的方法，比如可以对负载较高的机器分配更少的数据和计算量，PS-Core也为此设计了一个Backup
instance机制。当某个节点被确定为慢节点时，coordinator会把慢节点标记为"dead"节点，请求Fuxi重新调度一个新的节点作为该节点的备份节点，并将该节点的负载转移到备份节点上。这种机制通常可以带来10%-20%的效率提升。</p>
<p>KunPeng对不同稀疏度和不同数据类型的数据通信做了深度优化，并且提供workers之间点对点的通信接口，比如AllReduce，ReduceTo和Bcast，这些灵活的通信接口使得KunPeng可以拓展更多的功能，比如模型并行。</p>
<h2 id="ftrl">FTRL</h2>
<p><span
class="math display">\[w_{t+1}=\mathop{\arg\min}_{w}\left(\sum_{s=1}^{t}g_{s}w+\frac{1}{2}\sum_{s=1}^{t}\delta_{s}{\Vert}w-w_{s}{\Vert}_{2}^{2}+\lambda_{1}{\Vert}w{\Vert}_{1}+\lambda_{2}{\Vert}w{\Vert}_{2}^{2}\right)\]</span>
其中<span class="math inline">\(g\)</span>为损失函数对<span
class="math inline">\(w\)</span>的梯度，<span
class="math inline">\(\delta_{t}=\frac{1}{\eta_{t}}-\frac{1}{\eta_{t-1}}\)</span>，因此<span
class="math inline">\(\sum_{s=1}^{t}{\delta_{s}}=\frac{1}{\eta_{t}}\)</span>，<span
class="math inline">\(\eta\)</span>为学习率，并且<span
class="math inline">\(\eta_{t,i}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^{s}{g_{s,i}^2}}}\)</span>，通常<span
class="math inline">\(\alpha=1\)</span>，<span
class="math inline">\(\beta\)</span>是与数据集和特征相关的超参数。<span
class="math inline">\(\lambda_{1}\)</span>为L1系数，<span
class="math inline">\(\lambda_{2}\)</span>为L2系数。 更新公式为<br />
<span class="math display">\[w_{t+1}=\begin{cases}0&amp; if\
{\vert}z_{i}{\vert}{\leq}\lambda_{1}\\
-(\frac{\beta+\sqrt{n_{i}}}{\alpha}+\lambda_{2})^{-1}(z_{i}-sign(z_{i})\lambda_{1})&amp;
otherwise\end{cases}\]</span> 下图表明了LR
FTRL-Proximal算法单机更新过程。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/66cf72a181547ae24831af8500b47d72.png?raw=true%20=500"
alt="66cf72a181547ae24831af8500b47d72" />
<figcaption
aria-hidden="true">66cf72a181547ae24831af8500b47d72</figcaption>
</figure>
<p>这个算法在单机时很容易实现，但在分布式环境必须要考虑通信效率、servers的负载和算法收敛性问题。考虑到BSP的低效和ASP可能不收敛的问题，他们使用了bounded
delay的SSP更新方法，并且设置trust
region来调节参数范围，避免模型发散。整个算法具体过程如下：</p>
<ul>
<li>workers本地保存了模型<span class="math inline">\(w\)</span>和<span
class="math inline">\(z\)</span>、<span
class="math inline">\(n\)</span>，<span
class="math inline">\(z\)</span>、<span
class="math inline">\(n\)</span>通过bounded-asynchronous的方式与servers保持同步</li>
<li>workers加载数据，根据<span class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>更新本地模型<span
class="math inline">\(w\)</span>，计算梯度并更新本地模型<span
class="math inline">\(w\)</span>和<span
class="math inline">\(z\)</span>、<span
class="math inline">\(n\)</span>，同时使用<span
class="math inline">\(\delta_{z}\)</span>和<span
class="math inline">\(\delta_{n}\)</span>累加<span
class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>的增量，在需要与servers同步的时候将累加的<span
class="math inline">\(\delta_{z}\)</span>和<span
class="math inline">\(\delta_{n}\)</span> push到servers</li>
<li>servers合并所有workers发送的<span
class="math inline">\(\delta_{z}\)</span>和<span
class="math inline">\(\delta_{n}\)</span>，最后更新全局<span
class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>。</li>
</ul>
<p>workers向servers传递<span class="math inline">\(z\)</span>和<span
class="math inline">\(n\)</span>的增量，而不是直接传递模型梯度，这种做法虽然会带来一些通信开销，但降低了servers的计算负载，这是在通信效率和计算负载之间做的平衡。为了避免发散，servers在trust
region下更新模型。trust
region的策略有两种：一种是当模型中的元素超出置信阈时，直接回退整个模型；另一种是通过映射的方式将模型的值限制在置信阈中。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/0de2241d38a792bb79446944d65d8c66.png?raw=true%20=600"
alt="0de2241d38a792bb79446944d65d8c66" />
<figcaption
aria-hidden="true">0de2241d38a792bb79446944d65d8c66</figcaption>
</figure>
<h2 id="mart">MART</h2>
<p>MART（多增量回归树）又叫做GBDT，是一种应用比较广泛的机器学习算法。KunPeng实现了一个通用的MART算法，支持千亿级样本量和上千维的特征，并在MART的基础上实现了LambdaMART算法。</p>
<ul>
<li>MART
为了处理超大规模的数据量，KunPeng-MART使用数据并行的方式减少内存使用量，并采用了XGBoost的分布式加权直方图算法优化分裂点查找过程。具体来说就是，每个worker都保存了整颗树，在分割叶节点时，
（1）每个worker使用分配的数据子集计算一个局部加权直方图，计算完成后将直方图push到servers
（2）servers收到workers发送的直方图后，采用多路合并算法得到全局直方图，并找到最优分割点
（3）workers从servers
pull分割点，分裂节点并将数据分到分裂后的叶节点</li>
</ul>
<p>重复上述过程，可以得到整棵树。然后只要按照gradient
boosting方法一棵一棵地建树，最终得到MART。随着特征维度和树深度的增加，查找分裂点过程中的计算和通信都可能成为性能瓶颈。为了解决这个问题，他们提到使用KunPeng的通信模式去减少合并局部直方图的开销，但并没有透露具体的方法。</p>
<ul>
<li>LambdaMART
LambdaMART建树的过程与上面的MART一样，不同的是LambdaMART计算一阶导数和二阶导数的方式。由于LambdaMART要求同一个query
group的训练数据按sample两两组成pair对，因此当训练数据不是按照query
group连续存储时就会存在问题。对于这个问题，他们提出了两种解决方法：<br />
（1）先全局统计一下每个query id对应的样本总数，然后按照multiway number
partitioning algorithm对query
id进行分片，每个worker只加载属于自己的query ids对应的训练样本。<br />
（2）第二种是近似的方法。首先要求相同query
id的样本在文件系统中是连续存储的，然后每个worker还是按照正常情况加载属于自己的分片数据。如果相同query
id的样本被分在两个不同的worker上，则会把这两个worker上相同query
id的样本当做不同query id来处理。</li>
</ul>
<h2 id="其他算法">其他算法</h2>
<ul>
<li>Large-scale sparse Logistic Regression (LR)<br />
实现了不同的优化算法，L-BFGS、OWL-QN和BCD，其中BCD算法是数据和模型同时并行的算法。<br />
</li>
<li>Distributed Factorization Machines<br />
workers异步计算梯度，使用AdaGrad优化算法<br />
</li>
<li>Caffe<br />
实现了Caffe和KunPeng的对接，a generalized CPU-based large-scale deep
learning platform，简化DL算法开发</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<p>下面的实验都是在一个拥有5000台服务器的正式集群上进行的，每台机器12个Intel
Xeon CPU E5-2430 (2.2 GHz) CPU和96GB内存。</p>
<h3
id="kunpengspark和mpi的lr算法对比">KunPeng、Spark和MPI的LR算法对比</h3>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/143e082b7f1a6b54e47e9c8b51026dbb.png?raw=true"
alt="143e082b7f1a6b54e47e9c8b51026dbb" />
<figcaption
aria-hidden="true">143e082b7f1a6b54e47e9c8b51026dbb</figcaption>
</figure>
<p>不同平台的LR都采用L-BFGS算法更新，并且memory history
parameter都设置为10，并且使用同一个集群相同的CPU资源，在7个不同的数据集上KunPeng在效率和内存占用上都取得非常明显的优势。</p>
<p>在另外一个18 billion样本和 7
billion特征的数据集上，他们统计了KunPeng在不同workers数量时的加速比。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/00c84f368394ba04d59dbe530f69c387.png?raw=true"
alt="00c84f368394ba04d59dbe530f69c387" />
<figcaption
aria-hidden="true">00c84f368394ba04d59dbe530f69c387</figcaption>
</figure>
<p>KunPeng仅使用25个workers就可以训练这么大的数据，workers增加时依然能保持较高的加速比，并且内存占用随着workers增加而近乎直线降低。</p>
<h3 id="kunpeng-mart和xgboost的对比">KunPeng-MART和XGBoost的对比</h3>
<p>下图分别为KunPeng-MAR和XGBoost在不同任务上的峰值内存占用和训练时间对比。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/1b0888cab293242eaccdc2b6e5bf25d9.png?raw=true%20=500"
alt="1b0888cab293242eaccdc2b6e5bf25d9" />
<figcaption
aria-hidden="true">1b0888cab293242eaccdc2b6e5bf25d9</figcaption>
</figure>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/3b99dc82bc268d3da394a688c0234908.png?raw=true%20=500"
alt="3b99dc82bc268d3da394a688c0234908" />
<figcaption
aria-hidden="true">3b99dc82bc268d3da394a688c0234908</figcaption>
</figure>
<h3
id="kunpeng-fmlibfm和difacto的对比">KunPeng-FM、LibFM和DiFacto的对比</h3>
<p>下面是在单机情况下的训练效果对比，并没有训练时间的对比数据和多机实验相关的数据。</p>
<figure>
<img
src="https://github.com/hjchen2/personal/blob/master/blog/KunPeng/da511a1bb0db987fb74ebb08fa5352c9.png?raw=true%20=500"
alt="da511a1bb0db987fb74ebb08fa5352c9" />
<figcaption
aria-hidden="true">da511a1bb0db987fb74ebb08fa5352c9</figcaption>
</figure>
<h2 id="参考资料">参考资料</h2>
<p>1、Ad Click Prediction: a View from the Trenches.</p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/ML-framework/">ML framework</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/large-scale-ML-framework/">large scale ML framework</a><a href="/tags/KunPeng/">KunPeng</a>
  </div>

</div>



</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/12/01/seq2seq串讲/" title="NEURAL MACHINE TRANSLATION论文学习串讲">
  <strong>上一篇：</strong><br/>
  <span>
  NEURAL MACHINE TRANSLATION论文学习串讲</span>
</a>
</div>


<div class="next">
<a href="/2017/07/03/C++调用Python接口/"  title="C++调用python">
 <strong>下一篇：</strong><br/> 
 <span>C++调用python
</span>
</a>
</div>

</nav>

	



</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kunpeng%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">KunPeng整体架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#apsara-cloud-platform"><span class="toc-number">2.1.</span> <span class="toc-text">Apsara Cloud Platform</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kunpeng-platform"><span class="toc-number">3.</span> <span class="toc-text">KunPeng Platform</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E6%96%B9%E6%A1%88"><span class="toc-number">3.1.</span> <span class="toc-text">容错方案</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dag%E8%B0%83%E5%BA%A6"><span class="toc-number">3.2.</span> <span class="toc-text">DAG调度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E9%80%9A%E4%BF%A1%E6%8E%A5%E5%8F%A3"><span class="toc-number">3.3.</span> <span class="toc-text">负载均衡和通信接口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ftrl"><span class="toc-number">4.</span> <span class="toc-text">FTRL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mart"><span class="toc-number">5.</span> <span class="toc-text">MART</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">其他算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">7.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kunpengspark%E5%92%8Cmpi%E7%9A%84lr%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">7.1.</span> <span class="toc-text">KunPeng、Spark和MPI的LR算法对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kunpeng-mart%E5%92%8Cxgboost%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">7.2.</span> <span class="toc-text">KunPeng-MART和XGBoost的对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kunpeng-fmlibfm%E5%92%8Cdifacto%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">7.3.</span> <span class="toc-text">KunPeng-FM、LibFM和DiFacto的对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">8.</span> <span class="toc-text">参考资料</span></a></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  


  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/DL-Compiler/" title="DL Compiler">DL Compiler<sup>8</sup></a></li>
		  
		
		  
			<li><a href="/categories/Daily/" title="Daily">Daily<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/ML-framework/" title="ML framework">ML framework<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/XRT/" title="XRT">XRT<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/code/" title="code">code<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/deep-learning/" title="deep learning">deep learning<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/graph-optimization-图优化/" title="graph optimization, 图优化">graph optimization, 图优化<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/kaldi-decision-tree-决策树/" title="kaldi, decision tree, 决策树">kaldi, decision tree, 决策树<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/low-bitwidth/" title="low bitwidth">low bitwidth<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/model-compression/" title="model compression">model compression<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/neural-machine-translation/" title="neural machine translation">neural machine translation<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/tvm-knowledge/" title="tvm knowledge">tvm knowledge<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Deep-Learning-Compiler/" title="Deep Learning Compiler">Deep Learning Compiler<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/IREE/" title="IREE">IREE<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/reinforcement-learning/" title="reinforcement learning">reinforcement learning<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/caffe/" title="caffe">caffe<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/deep-learning/" title="deep learning">deep learning<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/machine-learning/" title="machine learning">machine learning<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/momentum/" title="momentum">momentum<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/XLA/" title="XLA">XLA<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/FusionStitching/" title="FusionStitching">FusionStitching<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/c/" title="c++">c++<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/embedding/" title="embedding">embedding<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/kaldi/" title="kaldi">kaldi<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/decision-tree/" title="decision tree">decision tree<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/决策树/" title="决策树">决策树<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/HMM/" title="HMM">HMM<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/上下文相关音素/" title="上下文相关音素">上下文相关音素<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/large-scale-ML-framework/" title="large scale ML framework">large scale ML framework<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/KunPeng/" title="KunPeng">KunPeng<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/web-technology/" title="web technology">web technology<sup>1</sup></a></li>
			
		
		</ul>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2023 
		
		<a href="/about" target="_blank" title="Dou Jiang">Dou Jiang</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
